{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Q Networks\n",
    "\n",
    "Code can be found at: https://www.analyticsvidhya.com/blog/2019/04/introduction-deep-q-learning-python/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import gym\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, Flatten\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "from rl.agents.dqn import DQNAgent\n",
    "from rl.policy import EpsGreedyQPolicy\n",
    "from rl.memory import SequentialMemory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the environment and extract the number of actions available\n",
    "env = gym.make('CartPole-v0')\n",
    "np.random.seed(0)\n",
    "env.seed(0)\n",
    "nb_actions = env.action_space.n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Create Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Richard\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "flatten_1 (Flatten)          (None, 4)                 0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 16)                80        \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 16)                0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 2)                 34        \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 2)                 0         \n",
      "=================================================================\n",
      "Total params: 114\n",
      "Trainable params: 114\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Flatten(input_shape=(1,) + env.observation_space.shape))\n",
    "model.add(Dense(16))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dense(nb_actions))\n",
    "model.add(Activation('linear'))\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Train and Test a simple DQN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initiate Parameters\n",
    "policy = EpsGreedyQPolicy()\n",
    "memory = SequentialMemory(limit=50000, window_length=1)\n",
    "dqn = DQNAgent(model=model, nb_actions=nb_actions, memory=memory, nb_steps_warmup=10,\n",
    "target_model_update=1e-2, policy=policy)\n",
    "dqn.compile(Adam(lr=1e-3), metrics=['mae'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train 5,000 Times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for 5000 steps ...\n",
      "    9/5000: episode: 1, duration: 0.328s, episode steps: 9, steps per second: 27, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.143 [-1.715, 2.789], loss: --, mean_absolute_error: --, mean_q: --\n",
      "WARNING:tensorflow:From C:\\Users\\Richard\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Richard\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\rl\\memory.py:39: UserWarning: Not enough entries to sample without replacement. Consider increasing your warm-up phase to avoid oversampling!\n",
      "  warnings.warn('Not enough entries to sample without replacement. Consider increasing your warm-up phase to avoid oversampling!')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   22/5000: episode: 2, duration: 0.567s, episode steps: 13, steps per second: 23, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.154 [0.000, 1.000], mean observation: 0.115 [-1.745, 2.863], loss: 0.623680, mean_absolute_error: 0.738524, mean_q: 0.574850\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Richard\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\rl\\memory.py:39: UserWarning: Not enough entries to sample without replacement. Consider increasing your warm-up phase to avoid oversampling!\n",
      "  warnings.warn('Not enough entries to sample without replacement. Consider increasing your warm-up phase to avoid oversampling!')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   34/5000: episode: 3, duration: 0.202s, episode steps: 12, steps per second: 59, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.083 [0.000, 1.000], mean observation: 0.112 [-1.950, 3.064], loss: 0.565361, mean_absolute_error: 0.744800, mean_q: 0.734636\n",
      "   44/5000: episode: 4, duration: 0.164s, episode steps: 10, steps per second: 61, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.100 [0.000, 1.000], mean observation: 0.141 [-1.557, 2.618], loss: 0.531535, mean_absolute_error: 0.739168, mean_q: 0.845040\n",
      "   54/5000: episode: 5, duration: 0.166s, episode steps: 10, steps per second: 60, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.100 [0.000, 1.000], mean observation: 0.142 [-1.716, 2.698], loss: 0.488719, mean_absolute_error: 0.715716, mean_q: 0.939681\n",
      "   63/5000: episode: 6, duration: 0.150s, episode steps: 9, steps per second: 60, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.133 [-1.808, 2.781], loss: 0.447721, mean_absolute_error: 0.699017, mean_q: 1.066571\n",
      "   72/5000: episode: 7, duration: 0.150s, episode steps: 9, steps per second: 60, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.111 [0.000, 1.000], mean observation: 0.155 [-1.537, 2.456], loss: 0.434595, mean_absolute_error: 0.685461, mean_q: 1.154371\n",
      "   84/5000: episode: 8, duration: 0.198s, episode steps: 12, steps per second: 61, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.083 [0.000, 1.000], mean observation: 0.117 [-1.979, 3.083], loss: 0.422335, mean_absolute_error: 0.665941, mean_q: 1.268815\n",
      "   94/5000: episode: 9, duration: 0.168s, episode steps: 10, steps per second: 60, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.159 [-1.915, 3.088], loss: 0.380120, mean_absolute_error: 0.620398, mean_q: 1.447947\n",
      "  103/5000: episode: 10, duration: 0.148s, episode steps: 9, steps per second: 61, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.137 [-1.776, 2.747], loss: 0.390617, mean_absolute_error: 0.592071, mean_q: 1.508334\n",
      "  115/5000: episode: 11, duration: 0.200s, episode steps: 12, steps per second: 60, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.083 [0.000, 1.000], mean observation: 0.113 [-1.906, 2.979], loss: 0.388374, mean_absolute_error: 0.563944, mean_q: 1.624060\n",
      "  123/5000: episode: 12, duration: 0.132s, episode steps: 8, steps per second: 60, episode reward: 8.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.162 [-1.592, 2.583], loss: 0.383039, mean_absolute_error: 0.537157, mean_q: 1.771578\n",
      "  134/5000: episode: 13, duration: 0.182s, episode steps: 11, steps per second: 60, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.091 [0.000, 1.000], mean observation: 0.121 [-1.717, 2.692], loss: 0.426934, mean_absolute_error: 0.522023, mean_q: 1.865745\n",
      "  144/5000: episode: 14, duration: 0.168s, episode steps: 10, steps per second: 60, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.143 [-1.973, 3.049], loss: 0.441403, mean_absolute_error: 0.520917, mean_q: 1.783704\n",
      "  152/5000: episode: 15, duration: 0.132s, episode steps: 8, steps per second: 60, episode reward: 8.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.160 [-1.525, 2.529], loss: 0.506210, mean_absolute_error: 0.564054, mean_q: 1.936856\n",
      "  163/5000: episode: 16, duration: 0.182s, episode steps: 11, steps per second: 60, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.091 [0.000, 1.000], mean observation: 0.131 [-1.797, 2.783], loss: 0.501842, mean_absolute_error: 0.518751, mean_q: 1.865936\n",
      "  172/5000: episode: 17, duration: 0.149s, episode steps: 9, steps per second: 60, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.143 [-1.805, 2.840], loss: 0.434193, mean_absolute_error: 0.476980, mean_q: 1.948778\n",
      "  183/5000: episode: 18, duration: 0.194s, episode steps: 11, steps per second: 57, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.091 [0.000, 1.000], mean observation: 0.147 [-1.734, 2.783], loss: 0.390185, mean_absolute_error: 0.451834, mean_q: 2.046971\n",
      "  194/5000: episode: 19, duration: 0.188s, episode steps: 11, steps per second: 59, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.091 [0.000, 1.000], mean observation: 0.097 [-1.774, 2.770], loss: 0.476649, mean_absolute_error: 0.494251, mean_q: 2.187201\n",
      "  203/5000: episode: 20, duration: 0.149s, episode steps: 9, steps per second: 60, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.170 [-1.713, 2.874], loss: 0.477452, mean_absolute_error: 0.552227, mean_q: 2.184871\n",
      "  216/5000: episode: 21, duration: 0.216s, episode steps: 13, steps per second: 60, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.154 [0.000, 1.000], mean observation: 0.101 [-1.732, 2.753], loss: 0.571739, mean_absolute_error: 0.603882, mean_q: 2.259889\n",
      "  226/5000: episode: 22, duration: 0.166s, episode steps: 10, steps per second: 60, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.100 [0.000, 1.000], mean observation: 0.152 [-1.557, 2.565], loss: 0.419452, mean_absolute_error: 0.600201, mean_q: 2.300636\n",
      "  236/5000: episode: 23, duration: 0.168s, episode steps: 10, steps per second: 60, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.130 [-1.955, 3.024], loss: 0.425805, mean_absolute_error: 0.639842, mean_q: 2.416410\n",
      "  245/5000: episode: 24, duration: 0.148s, episode steps: 9, steps per second: 61, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.150 [-1.773, 2.871], loss: 0.618124, mean_absolute_error: 0.734343, mean_q: 2.653100\n",
      "  255/5000: episode: 25, duration: 0.166s, episode steps: 10, steps per second: 60, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.100 [0.000, 1.000], mean observation: 0.145 [-1.536, 2.513], loss: 0.373546, mean_absolute_error: 0.680155, mean_q: 2.547954\n",
      "  264/5000: episode: 26, duration: 0.149s, episode steps: 9, steps per second: 61, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.141 [-1.782, 2.857], loss: 0.429393, mean_absolute_error: 0.739247, mean_q: 2.693106\n",
      "  274/5000: episode: 27, duration: 0.167s, episode steps: 10, steps per second: 60, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.100 [0.000, 1.000], mean observation: 0.116 [-1.583, 2.543], loss: 0.481073, mean_absolute_error: 0.796568, mean_q: 2.713394\n",
      "  284/5000: episode: 28, duration: 0.165s, episode steps: 10, steps per second: 61, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.135 [-1.905, 2.999], loss: 0.490466, mean_absolute_error: 0.866153, mean_q: 2.649461\n",
      "  294/5000: episode: 29, duration: 0.168s, episode steps: 10, steps per second: 59, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.153 [-1.934, 3.075], loss: 0.369979, mean_absolute_error: 0.809121, mean_q: 2.662012\n",
      "  303/5000: episode: 30, duration: 0.165s, episode steps: 9, steps per second: 54, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.159 [-1.763, 2.888], loss: 0.417321, mean_absolute_error: 0.881421, mean_q: 2.796083\n",
      "  312/5000: episode: 31, duration: 0.148s, episode steps: 9, steps per second: 61, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.142 [-1.775, 2.818], loss: 0.411095, mean_absolute_error: 0.902672, mean_q: 2.918329\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  323/5000: episode: 32, duration: 0.194s, episode steps: 11, steps per second: 57, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.091 [0.000, 1.000], mean observation: 0.140 [-1.712, 2.778], loss: 0.441408, mean_absolute_error: 0.972610, mean_q: 2.965413\n",
      "  335/5000: episode: 33, duration: 0.186s, episode steps: 12, steps per second: 65, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.083 [0.000, 1.000], mean observation: 0.079 [-2.003, 2.989], loss: 0.451643, mean_absolute_error: 0.977772, mean_q: 2.975425\n",
      "  349/5000: episode: 34, duration: 0.233s, episode steps: 14, steps per second: 60, episode reward: 14.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.143 [0.000, 1.000], mean observation: 0.078 [-2.001, 3.014], loss: 0.468996, mean_absolute_error: 1.040299, mean_q: 3.035325\n",
      "  359/5000: episode: 35, duration: 0.168s, episode steps: 10, steps per second: 59, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.118 [-1.986, 3.044], loss: 0.434520, mean_absolute_error: 1.035922, mean_q: 3.014664\n",
      "  368/5000: episode: 36, duration: 0.149s, episode steps: 9, steps per second: 60, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.146 [-1.745, 2.772], loss: 0.482914, mean_absolute_error: 1.082336, mean_q: 3.087739\n",
      "  377/5000: episode: 37, duration: 0.147s, episode steps: 9, steps per second: 61, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.121 [-1.805, 2.800], loss: 0.388135, mean_absolute_error: 1.086089, mean_q: 3.115076\n",
      "  386/5000: episode: 38, duration: 0.152s, episode steps: 9, steps per second: 59, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.155 [-1.724, 2.774], loss: 0.442268, mean_absolute_error: 1.130423, mean_q: 3.220105\n",
      "  396/5000: episode: 39, duration: 0.164s, episode steps: 10, steps per second: 61, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.127 [-1.984, 3.071], loss: 0.382135, mean_absolute_error: 1.137462, mean_q: 3.310870\n",
      "  405/5000: episode: 40, duration: 0.150s, episode steps: 9, steps per second: 60, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.111 [0.000, 1.000], mean observation: 0.136 [-1.365, 2.202], loss: 0.437208, mean_absolute_error: 1.179625, mean_q: 3.374513\n",
      "  414/5000: episode: 41, duration: 0.152s, episode steps: 9, steps per second: 59, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.146 [-1.713, 2.798], loss: 0.410499, mean_absolute_error: 1.214537, mean_q: 3.303078\n",
      "  423/5000: episode: 42, duration: 0.147s, episode steps: 9, steps per second: 61, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.135 [-1.741, 2.791], loss: 0.400236, mean_absolute_error: 1.245281, mean_q: 3.361309\n",
      "  433/5000: episode: 43, duration: 0.171s, episode steps: 10, steps per second: 58, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.134 [-1.979, 3.040], loss: 0.392883, mean_absolute_error: 1.262877, mean_q: 3.464500\n",
      "  442/5000: episode: 44, duration: 0.144s, episode steps: 9, steps per second: 62, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.173 [-1.741, 2.845], loss: 0.433333, mean_absolute_error: 1.317052, mean_q: 3.530308\n",
      "  452/5000: episode: 45, duration: 0.167s, episode steps: 10, steps per second: 60, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.145 [-1.913, 3.044], loss: 0.432167, mean_absolute_error: 1.388111, mean_q: 3.525784\n",
      "  462/5000: episode: 46, duration: 0.166s, episode steps: 10, steps per second: 60, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.142 [-1.929, 3.022], loss: 0.436570, mean_absolute_error: 1.423612, mean_q: 3.577798\n",
      "  472/5000: episode: 47, duration: 0.166s, episode steps: 10, steps per second: 60, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.130 [-1.968, 3.061], loss: 0.417877, mean_absolute_error: 1.456030, mean_q: 3.668104\n",
      "  486/5000: episode: 48, duration: 0.231s, episode steps: 14, steps per second: 61, episode reward: 14.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.214 [0.000, 1.000], mean observation: 0.103 [-1.552, 2.597], loss: 0.348456, mean_absolute_error: 1.440118, mean_q: 3.745817\n",
      "  497/5000: episode: 49, duration: 0.183s, episode steps: 11, steps per second: 60, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.091 [0.000, 1.000], mean observation: 0.118 [-1.786, 2.763], loss: 0.365170, mean_absolute_error: 1.483196, mean_q: 3.789133\n",
      "  505/5000: episode: 50, duration: 0.133s, episode steps: 8, steps per second: 60, episode reward: 8.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.143 [-1.618, 2.596], loss: 0.411499, mean_absolute_error: 1.568360, mean_q: 3.823817\n",
      "  514/5000: episode: 51, duration: 0.150s, episode steps: 9, steps per second: 60, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.161 [-1.734, 2.871], loss: 0.355813, mean_absolute_error: 1.575982, mean_q: 3.859817\n",
      "  522/5000: episode: 52, duration: 0.134s, episode steps: 8, steps per second: 60, episode reward: 8.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.143 [-1.555, 2.555], loss: 0.387405, mean_absolute_error: 1.615268, mean_q: 3.945372\n",
      "  532/5000: episode: 53, duration: 0.164s, episode steps: 10, steps per second: 61, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.143 [-1.956, 3.119], loss: 0.329792, mean_absolute_error: 1.614284, mean_q: 4.010697\n",
      "  540/5000: episode: 54, duration: 0.135s, episode steps: 8, steps per second: 59, episode reward: 8.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.151 [-1.532, 2.544], loss: 0.358604, mean_absolute_error: 1.644441, mean_q: 4.012046\n",
      "  548/5000: episode: 55, duration: 0.131s, episode steps: 8, steps per second: 61, episode reward: 8.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.163 [-1.540, 2.554], loss: 0.415607, mean_absolute_error: 1.711645, mean_q: 4.013716\n",
      "  557/5000: episode: 56, duration: 0.149s, episode steps: 9, steps per second: 60, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.144 [-1.775, 2.822], loss: 0.333211, mean_absolute_error: 1.689824, mean_q: 4.114456\n",
      "  569/5000: episode: 57, duration: 0.200s, episode steps: 12, steps per second: 60, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.083 [0.000, 1.000], mean observation: 0.141 [-1.935, 3.090], loss: 0.349264, mean_absolute_error: 1.735575, mean_q: 4.125664\n",
      "  580/5000: episode: 58, duration: 0.183s, episode steps: 11, steps per second: 60, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.091 [0.000, 1.000], mean observation: 0.094 [-1.803, 2.684], loss: 0.357521, mean_absolute_error: 1.757079, mean_q: 4.246677\n",
      "  590/5000: episode: 59, duration: 0.169s, episode steps: 10, steps per second: 59, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.200 [0.000, 1.000], mean observation: 0.126 [-1.412, 2.210], loss: 0.411499, mean_absolute_error: 1.819399, mean_q: 4.222257\n",
      "  600/5000: episode: 60, duration: 0.162s, episode steps: 10, steps per second: 62, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.158 [-1.956, 3.101], loss: 0.344902, mean_absolute_error: 1.815041, mean_q: 4.198482\n",
      "  610/5000: episode: 61, duration: 0.169s, episode steps: 10, steps per second: 59, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.141 [-1.984, 3.056], loss: 0.306564, mean_absolute_error: 1.809394, mean_q: 4.374177\n",
      "  620/5000: episode: 62, duration: 0.164s, episode steps: 10, steps per second: 61, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.100 [0.000, 1.000], mean observation: 0.132 [-1.554, 2.578], loss: 0.285300, mean_absolute_error: 1.838305, mean_q: 4.433080\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  630/5000: episode: 63, duration: 0.173s, episode steps: 10, steps per second: 58, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.139 [-1.955, 3.035], loss: 0.348101, mean_absolute_error: 1.885573, mean_q: 4.439894\n",
      "  640/5000: episode: 64, duration: 0.158s, episode steps: 10, steps per second: 63, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.100 [0.000, 1.000], mean observation: 0.150 [-1.735, 2.755], loss: 0.306011, mean_absolute_error: 1.884556, mean_q: 4.390641\n",
      "  650/5000: episode: 65, duration: 0.167s, episode steps: 10, steps per second: 60, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.123 [-1.938, 2.947], loss: 0.290853, mean_absolute_error: 1.893733, mean_q: 4.443768\n",
      "  659/5000: episode: 66, duration: 0.150s, episode steps: 9, steps per second: 60, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.111 [0.000, 1.000], mean observation: 0.140 [-1.609, 2.505], loss: 0.369834, mean_absolute_error: 1.968900, mean_q: 4.491037\n",
      "  669/5000: episode: 67, duration: 0.165s, episode steps: 10, steps per second: 61, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.100 [0.000, 1.000], mean observation: 0.153 [-1.544, 2.616], loss: 0.432333, mean_absolute_error: 2.018197, mean_q: 4.433709\n",
      "  677/5000: episode: 68, duration: 0.134s, episode steps: 8, steps per second: 60, episode reward: 8.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.147 [-1.582, 2.512], loss: 0.274713, mean_absolute_error: 1.963108, mean_q: 4.519352\n",
      "  687/5000: episode: 69, duration: 0.166s, episode steps: 10, steps per second: 60, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.100 [0.000, 1.000], mean observation: 0.125 [-1.594, 2.617], loss: 0.267359, mean_absolute_error: 1.977927, mean_q: 4.525154\n",
      "  696/5000: episode: 70, duration: 0.149s, episode steps: 9, steps per second: 60, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.111 [0.000, 1.000], mean observation: 0.134 [-1.343, 2.278], loss: 0.260732, mean_absolute_error: 1.998870, mean_q: 4.728755\n",
      "  706/5000: episode: 71, duration: 0.166s, episode steps: 10, steps per second: 60, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.100 [0.000, 1.000], mean observation: 0.113 [-1.751, 2.646], loss: 0.314198, mean_absolute_error: 2.059879, mean_q: 4.737309\n",
      "  717/5000: episode: 72, duration: 0.184s, episode steps: 11, steps per second: 60, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.182 [0.000, 1.000], mean observation: 0.123 [-1.591, 2.443], loss: 0.418442, mean_absolute_error: 2.148172, mean_q: 4.679272\n",
      "  727/5000: episode: 73, duration: 0.166s, episode steps: 10, steps per second: 60, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.200 [0.000, 1.000], mean observation: 0.112 [-1.414, 2.238], loss: 0.299295, mean_absolute_error: 2.116457, mean_q: 4.591311\n",
      "  737/5000: episode: 74, duration: 0.165s, episode steps: 10, steps per second: 60, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.200 [0.000, 1.000], mean observation: 0.117 [-1.406, 2.206], loss: 0.239844, mean_absolute_error: 2.094161, mean_q: 4.694734\n",
      "  746/5000: episode: 75, duration: 0.150s, episode steps: 9, steps per second: 60, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.222 [0.000, 1.000], mean observation: 0.146 [-0.952, 1.768], loss: 0.311960, mean_absolute_error: 2.178540, mean_q: 4.861860\n",
      "  756/5000: episode: 76, duration: 0.166s, episode steps: 10, steps per second: 60, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.100 [0.000, 1.000], mean observation: 0.140 [-1.556, 2.454], loss: 0.230960, mean_absolute_error: 2.161682, mean_q: 4.949034\n",
      "  769/5000: episode: 77, duration: 0.222s, episode steps: 13, steps per second: 59, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.231 [0.000, 1.000], mean observation: 0.093 [-1.515, 2.356], loss: 0.256141, mean_absolute_error: 2.210967, mean_q: 5.047368\n",
      "  779/5000: episode: 78, duration: 0.160s, episode steps: 10, steps per second: 62, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.200 [0.000, 1.000], mean observation: 0.136 [-1.511, 2.395], loss: 0.307069, mean_absolute_error: 2.242675, mean_q: 4.975751\n",
      "  789/5000: episode: 79, duration: 0.174s, episode steps: 10, steps per second: 58, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.200 [0.000, 1.000], mean observation: 0.132 [-1.327, 2.165], loss: 0.309835, mean_absolute_error: 2.265152, mean_q: 4.900313\n",
      "  799/5000: episode: 80, duration: 0.157s, episode steps: 10, steps per second: 64, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.200 [0.000, 1.000], mean observation: 0.110 [-1.575, 2.358], loss: 0.313414, mean_absolute_error: 2.306272, mean_q: 4.973745\n",
      "  812/5000: episode: 81, duration: 0.216s, episode steps: 13, steps per second: 60, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.308 [0.000, 1.000], mean observation: 0.073 [-1.411, 2.123], loss: 0.380789, mean_absolute_error: 2.349764, mean_q: 4.933909\n",
      "  821/5000: episode: 82, duration: 0.149s, episode steps: 9, steps per second: 60, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.222 [0.000, 1.000], mean observation: 0.130 [-1.351, 2.160], loss: 0.338008, mean_absolute_error: 2.360723, mean_q: 4.970719\n",
      "  831/5000: episode: 83, duration: 0.166s, episode steps: 10, steps per second: 60, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.200 [0.000, 1.000], mean observation: 0.132 [-1.384, 2.176], loss: 0.289567, mean_absolute_error: 2.386980, mean_q: 5.080429\n",
      "  843/5000: episode: 84, duration: 0.204s, episode steps: 12, steps per second: 59, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.250 [0.000, 1.000], mean observation: 0.101 [-1.332, 2.097], loss: 0.269296, mean_absolute_error: 2.424634, mean_q: 5.282794\n",
      "  853/5000: episode: 85, duration: 0.165s, episode steps: 10, steps per second: 61, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.200 [0.000, 1.000], mean observation: 0.139 [-1.319, 2.174], loss: 0.277514, mean_absolute_error: 2.420052, mean_q: 5.182811\n",
      "  863/5000: episode: 86, duration: 0.165s, episode steps: 10, steps per second: 61, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.200 [0.000, 1.000], mean observation: 0.149 [-1.327, 2.094], loss: 0.281892, mean_absolute_error: 2.482023, mean_q: 5.287115\n",
      "  874/5000: episode: 87, duration: 0.183s, episode steps: 11, steps per second: 60, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.273 [0.000, 1.000], mean observation: 0.115 [-1.403, 2.108], loss: 0.315985, mean_absolute_error: 2.552616, mean_q: 5.408566\n",
      "  885/5000: episode: 88, duration: 0.183s, episode steps: 11, steps per second: 60, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.273 [0.000, 1.000], mean observation: 0.107 [-1.376, 2.032], loss: 0.312893, mean_absolute_error: 2.552914, mean_q: 5.304084\n",
      "  894/5000: episode: 89, duration: 0.150s, episode steps: 9, steps per second: 60, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.222 [0.000, 1.000], mean observation: 0.142 [-1.173, 1.956], loss: 0.308348, mean_absolute_error: 2.568340, mean_q: 5.360059\n",
      "  903/5000: episode: 90, duration: 0.149s, episode steps: 9, steps per second: 60, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.222 [0.000, 1.000], mean observation: 0.142 [-1.178, 1.968], loss: 0.230302, mean_absolute_error: 2.567569, mean_q: 5.457232\n",
      "  914/5000: episode: 91, duration: 0.179s, episode steps: 11, steps per second: 61, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.273 [0.000, 1.000], mean observation: 0.144 [-1.353, 2.080], loss: 0.282236, mean_absolute_error: 2.598661, mean_q: 5.508559\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  925/5000: episode: 92, duration: 0.183s, episode steps: 11, steps per second: 60, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.273 [0.000, 1.000], mean observation: 0.137 [-1.173, 1.929], loss: 0.252245, mean_absolute_error: 2.588031, mean_q: 5.470732\n",
      "  934/5000: episode: 93, duration: 0.156s, episode steps: 9, steps per second: 58, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.222 [0.000, 1.000], mean observation: 0.144 [-1.353, 2.133], loss: 0.189928, mean_absolute_error: 2.608366, mean_q: 5.637427\n",
      "  945/5000: episode: 94, duration: 0.179s, episode steps: 11, steps per second: 62, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.273 [0.000, 1.000], mean observation: 0.105 [-1.373, 2.123], loss: 0.357427, mean_absolute_error: 2.609134, mean_q: 5.482111\n",
      "  955/5000: episode: 95, duration: 0.166s, episode steps: 10, steps per second: 60, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.200 [0.000, 1.000], mean observation: 0.140 [-1.345, 2.083], loss: 0.324247, mean_absolute_error: 2.615877, mean_q: 5.453409\n",
      "  965/5000: episode: 96, duration: 0.165s, episode steps: 10, steps per second: 61, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.200 [0.000, 1.000], mean observation: 0.135 [-1.409, 2.329], loss: 0.279100, mean_absolute_error: 2.676142, mean_q: 5.639452\n",
      "  979/5000: episode: 97, duration: 0.230s, episode steps: 14, steps per second: 61, episode reward: 14.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.286 [0.000, 1.000], mean observation: 0.115 [-1.312, 2.007], loss: 0.334840, mean_absolute_error: 2.673296, mean_q: 5.556399\n",
      "  988/5000: episode: 98, duration: 0.154s, episode steps: 9, steps per second: 58, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.222 [0.000, 1.000], mean observation: 0.134 [-1.173, 1.925], loss: 0.269083, mean_absolute_error: 2.673724, mean_q: 5.593041\n",
      "  997/5000: episode: 99, duration: 0.148s, episode steps: 9, steps per second: 61, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.222 [0.000, 1.000], mean observation: 0.142 [-1.213, 1.978], loss: 0.281106, mean_absolute_error: 2.708444, mean_q: 5.650556\n",
      " 1006/5000: episode: 100, duration: 0.149s, episode steps: 9, steps per second: 60, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.222 [0.000, 1.000], mean observation: 0.126 [-1.396, 2.112], loss: 0.218578, mean_absolute_error: 2.673718, mean_q: 5.624571\n",
      " 1015/5000: episode: 101, duration: 0.149s, episode steps: 9, steps per second: 60, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.222 [0.000, 1.000], mean observation: 0.150 [-1.195, 1.964], loss: 0.256457, mean_absolute_error: 2.714679, mean_q: 5.618724\n",
      " 1026/5000: episode: 102, duration: 0.180s, episode steps: 11, steps per second: 61, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.273 [0.000, 1.000], mean observation: 0.093 [-1.222, 1.836], loss: 0.311720, mean_absolute_error: 2.780619, mean_q: 5.721039\n",
      " 1036/5000: episode: 103, duration: 0.166s, episode steps: 10, steps per second: 60, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.300 [0.000, 1.000], mean observation: 0.131 [-1.138, 1.875], loss: 0.333883, mean_absolute_error: 2.727694, mean_q: 5.559056\n",
      " 1045/5000: episode: 104, duration: 0.149s, episode steps: 9, steps per second: 60, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.222 [0.000, 1.000], mean observation: 0.121 [-1.218, 1.922], loss: 0.249883, mean_absolute_error: 2.756022, mean_q: 5.650632\n",
      " 1055/5000: episode: 105, duration: 0.171s, episode steps: 10, steps per second: 58, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.200 [0.000, 1.000], mean observation: 0.156 [-1.147, 2.025], loss: 0.294188, mean_absolute_error: 2.810783, mean_q: 5.747605\n",
      " 1064/5000: episode: 106, duration: 0.143s, episode steps: 9, steps per second: 63, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.222 [0.000, 1.000], mean observation: 0.141 [-1.210, 1.965], loss: 0.273167, mean_absolute_error: 2.809237, mean_q: 5.797047\n",
      " 1075/5000: episode: 107, duration: 0.186s, episode steps: 11, steps per second: 59, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.273 [0.000, 1.000], mean observation: 0.101 [-1.219, 1.930], loss: 0.256264, mean_absolute_error: 2.783386, mean_q: 5.751471\n",
      " 1085/5000: episode: 108, duration: 0.164s, episode steps: 10, steps per second: 61, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.300 [0.000, 1.000], mean observation: 0.131 [-1.354, 2.063], loss: 0.261873, mean_absolute_error: 2.819526, mean_q: 5.835923\n",
      " 1096/5000: episode: 109, duration: 0.188s, episode steps: 11, steps per second: 59, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.273 [0.000, 1.000], mean observation: 0.128 [-1.321, 2.040], loss: 0.268047, mean_absolute_error: 2.739922, mean_q: 5.556287\n",
      " 1105/5000: episode: 110, duration: 0.148s, episode steps: 9, steps per second: 61, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.222 [0.000, 1.000], mean observation: 0.136 [-1.179, 1.907], loss: 0.321521, mean_absolute_error: 2.837018, mean_q: 5.773844\n",
      " 1115/5000: episode: 111, duration: 0.159s, episode steps: 10, steps per second: 63, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.200 [0.000, 1.000], mean observation: 0.118 [-1.199, 1.976], loss: 0.283619, mean_absolute_error: 2.871117, mean_q: 5.853921\n",
      " 1130/5000: episode: 112, duration: 0.251s, episode steps: 15, steps per second: 60, episode reward: 15.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.333 [0.000, 1.000], mean observation: 0.085 [-1.329, 1.992], loss: 0.284406, mean_absolute_error: 2.871960, mean_q: 5.821287\n",
      " 1140/5000: episode: 113, duration: 0.165s, episode steps: 10, steps per second: 61, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.300 [0.000, 1.000], mean observation: 0.134 [-1.170, 1.861], loss: 0.277774, mean_absolute_error: 2.888784, mean_q: 5.855433\n",
      " 1149/5000: episode: 114, duration: 0.150s, episode steps: 9, steps per second: 60, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.222 [0.000, 1.000], mean observation: 0.133 [-1.194, 1.890], loss: 0.231704, mean_absolute_error: 2.997535, mean_q: 6.189275\n",
      " 1159/5000: episode: 115, duration: 0.172s, episode steps: 10, steps per second: 58, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.200 [0.000, 1.000], mean observation: 0.134 [-1.380, 2.167], loss: 0.278766, mean_absolute_error: 2.946578, mean_q: 5.984008\n",
      " 1168/5000: episode: 116, duration: 0.143s, episode steps: 9, steps per second: 63, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.222 [0.000, 1.000], mean observation: 0.154 [-1.136, 1.924], loss: 0.295737, mean_absolute_error: 2.966603, mean_q: 6.006112\n",
      " 1178/5000: episode: 117, duration: 0.165s, episode steps: 10, steps per second: 61, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.300 [0.000, 1.000], mean observation: 0.121 [-1.159, 1.852], loss: 0.302297, mean_absolute_error: 2.923816, mean_q: 5.820609\n",
      " 1189/5000: episode: 118, duration: 0.184s, episode steps: 11, steps per second: 60, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.273 [0.000, 1.000], mean observation: 0.115 [-1.330, 2.041], loss: 0.263385, mean_absolute_error: 3.021435, mean_q: 6.152337\n",
      " 1199/5000: episode: 119, duration: 0.167s, episode steps: 10, steps per second: 60, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.300 [0.000, 1.000], mean observation: 0.147 [-1.147, 1.904], loss: 0.242375, mean_absolute_error: 3.080933, mean_q: 6.268861\n",
      " 1208/5000: episode: 120, duration: 0.155s, episode steps: 9, steps per second: 58, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.222 [0.000, 1.000], mean observation: 0.139 [-1.168, 1.898], loss: 0.287113, mean_absolute_error: 2.978251, mean_q: 5.955400\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 1220/5000: episode: 121, duration: 0.208s, episode steps: 12, steps per second: 58, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.333 [0.000, 1.000], mean observation: 0.111 [-1.207, 1.855], loss: 0.327599, mean_absolute_error: 3.023829, mean_q: 6.018233\n",
      " 1233/5000: episode: 122, duration: 0.228s, episode steps: 13, steps per second: 57, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.308 [0.000, 1.000], mean observation: 0.091 [-1.184, 1.854], loss: 0.281513, mean_absolute_error: 3.037763, mean_q: 6.079416\n",
      " 1245/5000: episode: 123, duration: 0.190s, episode steps: 12, steps per second: 63, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.333 [0.000, 1.000], mean observation: 0.117 [-1.181, 1.878], loss: 0.209909, mean_absolute_error: 3.060978, mean_q: 6.189703\n",
      " 1254/5000: episode: 124, duration: 0.148s, episode steps: 9, steps per second: 61, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.222 [0.000, 1.000], mean observation: 0.153 [-1.002, 1.785], loss: 0.338865, mean_absolute_error: 3.120226, mean_q: 6.183992\n",
      " 1265/5000: episode: 125, duration: 0.185s, episode steps: 11, steps per second: 59, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.273 [0.000, 1.000], mean observation: 0.112 [-1.145, 1.846], loss: 0.197456, mean_absolute_error: 3.051575, mean_q: 6.107898\n",
      " 1274/5000: episode: 126, duration: 0.143s, episode steps: 9, steps per second: 63, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.222 [0.000, 1.000], mean observation: 0.128 [-0.976, 1.715], loss: 0.260736, mean_absolute_error: 3.091556, mean_q: 6.134587\n",
      " 1283/5000: episode: 127, duration: 0.150s, episode steps: 9, steps per second: 60, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.222 [0.000, 1.000], mean observation: 0.113 [-1.014, 1.686], loss: 0.199096, mean_absolute_error: 3.122078, mean_q: 6.269656\n",
      " 1295/5000: episode: 128, duration: 0.198s, episode steps: 12, steps per second: 61, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.333 [0.000, 1.000], mean observation: 0.094 [-1.181, 1.712], loss: 0.228433, mean_absolute_error: 3.139669, mean_q: 6.205656\n",
      " 1306/5000: episode: 129, duration: 0.191s, episode steps: 11, steps per second: 58, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.273 [0.000, 1.000], mean observation: 0.102 [-1.197, 1.910], loss: 0.224806, mean_absolute_error: 3.111104, mean_q: 6.128577\n",
      " 1316/5000: episode: 130, duration: 0.156s, episode steps: 10, steps per second: 64, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.300 [0.000, 1.000], mean observation: 0.112 [-1.028, 1.548], loss: 0.286539, mean_absolute_error: 3.072137, mean_q: 5.955747\n",
      " 1328/5000: episode: 131, duration: 0.200s, episode steps: 12, steps per second: 60, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.417 [0.000, 1.000], mean observation: 0.094 [-0.840, 1.418], loss: 0.236839, mean_absolute_error: 3.143783, mean_q: 6.114061\n",
      " 1340/5000: episode: 132, duration: 0.210s, episode steps: 12, steps per second: 57, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.333 [0.000, 1.000], mean observation: 0.103 [-0.984, 1.459], loss: 0.252775, mean_absolute_error: 3.212585, mean_q: 6.235596\n",
      " 1355/5000: episode: 133, duration: 0.236s, episode steps: 15, steps per second: 64, episode reward: 15.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.333 [0.000, 1.000], mean observation: 0.090 [-1.006, 1.731], loss: 0.173580, mean_absolute_error: 3.274138, mean_q: 6.405128\n",
      " 1369/5000: episode: 134, duration: 0.233s, episode steps: 14, steps per second: 60, episode reward: 14.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.429 [0.000, 1.000], mean observation: 0.102 [-0.773, 1.254], loss: 0.268725, mean_absolute_error: 3.311684, mean_q: 6.388600\n",
      " 1386/5000: episode: 135, duration: 0.284s, episode steps: 17, steps per second: 60, episode reward: 17.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.471 [0.000, 1.000], mean observation: 0.076 [-0.796, 1.247], loss: 0.308674, mean_absolute_error: 3.313173, mean_q: 6.308604\n",
      " 1400/5000: episode: 136, duration: 0.231s, episode steps: 14, steps per second: 61, episode reward: 14.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.429 [0.000, 1.000], mean observation: 0.077 [-0.808, 1.214], loss: 0.263061, mean_absolute_error: 3.273762, mean_q: 6.205728\n",
      " 1413/5000: episode: 137, duration: 0.216s, episode steps: 13, steps per second: 60, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.462 [0.000, 1.000], mean observation: 0.116 [-0.555, 1.095], loss: 0.301601, mean_absolute_error: 3.277452, mean_q: 6.216130\n",
      " 1430/5000: episode: 138, duration: 0.286s, episode steps: 17, steps per second: 60, episode reward: 17.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.471 [0.000, 1.000], mean observation: 0.085 [-0.576, 1.014], loss: 0.233781, mean_absolute_error: 3.372924, mean_q: 6.407581\n",
      " 1461/5000: episode: 139, duration: 0.513s, episode steps: 31, steps per second: 60, episode reward: 31.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.613 [0.000, 1.000], mean observation: -0.061 [-2.340, 1.360], loss: 0.198134, mean_absolute_error: 3.500449, mean_q: 6.729742\n",
      " 1471/5000: episode: 140, duration: 0.166s, episode steps: 10, steps per second: 60, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.900 [0.000, 1.000], mean observation: -0.121 [-2.706, 1.761], loss: 0.236442, mean_absolute_error: 3.504989, mean_q: 6.750968\n",
      " 1482/5000: episode: 141, duration: 0.182s, episode steps: 11, steps per second: 60, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.909 [0.000, 1.000], mean observation: -0.150 [-2.832, 1.715], loss: 0.169400, mean_absolute_error: 3.643560, mean_q: 7.031811\n",
      " 1491/5000: episode: 142, duration: 0.150s, episode steps: 9, steps per second: 60, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.179 [-2.891, 1.722], loss: 0.198038, mean_absolute_error: 3.634460, mean_q: 7.042851\n",
      " 1500/5000: episode: 143, duration: 0.152s, episode steps: 9, steps per second: 59, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.130 [-2.762, 1.807], loss: 0.262973, mean_absolute_error: 3.683199, mean_q: 7.100543\n",
      " 1511/5000: episode: 144, duration: 0.180s, episode steps: 11, steps per second: 61, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.156 [-3.319, 2.102], loss: 0.662605, mean_absolute_error: 3.733038, mean_q: 7.132945\n",
      " 1521/5000: episode: 145, duration: 0.166s, episode steps: 10, steps per second: 60, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.135 [-3.060, 1.988], loss: 1.296130, mean_absolute_error: 3.897108, mean_q: 7.447978\n",
      " 1532/5000: episode: 146, duration: 0.199s, episode steps: 11, steps per second: 55, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.113 [-3.289, 2.194], loss: 1.070179, mean_absolute_error: 3.792935, mean_q: 7.247832\n",
      " 1541/5000: episode: 147, duration: 0.151s, episode steps: 9, steps per second: 60, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.160 [-2.781, 1.715], loss: 2.052234, mean_absolute_error: 4.049723, mean_q: 7.555390\n",
      " 1551/5000: episode: 148, duration: 0.166s, episode steps: 10, steps per second: 60, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.125 [-2.972, 1.928], loss: 1.183517, mean_absolute_error: 3.972423, mean_q: 7.477209\n",
      " 1560/5000: episode: 149, duration: 0.151s, episode steps: 9, steps per second: 60, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.164 [-2.858, 1.737], loss: 3.145577, mean_absolute_error: 4.074935, mean_q: 7.357071\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 1570/5000: episode: 150, duration: 0.162s, episode steps: 10, steps per second: 62, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.132 [-2.986, 1.952], loss: 1.627133, mean_absolute_error: 4.098371, mean_q: 7.525575\n",
      " 1583/5000: episode: 151, duration: 0.220s, episode steps: 13, steps per second: 59, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.923 [0.000, 1.000], mean observation: -0.085 [-3.256, 2.187], loss: 0.739658, mean_absolute_error: 3.937388, mean_q: 7.252093\n",
      " 1598/5000: episode: 152, duration: 0.245s, episode steps: 15, steps per second: 61, episode reward: 15.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.400 [0.000, 1.000], mean observation: 0.085 [-0.818, 1.462], loss: 0.828060, mean_absolute_error: 3.952991, mean_q: 7.427989\n",
      " 1612/5000: episode: 153, duration: 0.237s, episode steps: 14, steps per second: 59, episode reward: 14.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.429 [0.000, 1.000], mean observation: 0.095 [-0.643, 1.203], loss: 0.935879, mean_absolute_error: 3.902076, mean_q: 7.332153\n",
      " 1625/5000: episode: 154, duration: 0.212s, episode steps: 13, steps per second: 61, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.385 [0.000, 1.000], mean observation: 0.083 [-0.836, 1.401], loss: 1.039612, mean_absolute_error: 3.939119, mean_q: 7.316878\n",
      " 1637/5000: episode: 155, duration: 0.199s, episode steps: 12, steps per second: 60, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.333 [0.000, 1.000], mean observation: 0.101 [-0.821, 1.448], loss: 1.900047, mean_absolute_error: 3.971181, mean_q: 7.355968\n",
      " 1650/5000: episode: 156, duration: 0.217s, episode steps: 13, steps per second: 60, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.385 [0.000, 1.000], mean observation: 0.116 [-0.934, 1.487], loss: 0.919194, mean_absolute_error: 3.981813, mean_q: 7.499535\n",
      " 1666/5000: episode: 157, duration: 0.267s, episode steps: 16, steps per second: 60, episode reward: 16.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.438 [0.000, 1.000], mean observation: 0.066 [-0.603, 1.143], loss: 1.092369, mean_absolute_error: 3.994397, mean_q: 7.460765\n",
      " 1690/5000: episode: 158, duration: 0.398s, episode steps: 24, steps per second: 60, episode reward: 24.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.750 [0.000, 1.000], mean observation: -0.009 [-3.266, 2.327], loss: 1.111253, mean_absolute_error: 4.097983, mean_q: 7.690941\n",
      " 1734/5000: episode: 159, duration: 0.734s, episode steps: 44, steps per second: 60, episode reward: 44.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.682 [0.000, 1.000], mean observation: 0.089 [-3.543, 3.110], loss: 1.486401, mean_absolute_error: 4.150710, mean_q: 7.687640\n",
      " 1777/5000: episode: 160, duration: 0.741s, episode steps: 43, steps per second: 58, episode reward: 43.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.581 [0.000, 1.000], mean observation: 0.116 [-1.526, 1.529], loss: 1.881258, mean_absolute_error: 4.218785, mean_q: 7.787523\n",
      " 1795/5000: episode: 161, duration: 0.289s, episode steps: 18, steps per second: 62, episode reward: 18.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.092 [-0.548, 1.044], loss: 1.509319, mean_absolute_error: 4.224351, mean_q: 7.723612\n",
      " 1813/5000: episode: 162, duration: 0.300s, episode steps: 18, steps per second: 60, episode reward: 18.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.064 [-0.634, 1.038], loss: 1.363026, mean_absolute_error: 4.168539, mean_q: 7.780499\n",
      " 1853/5000: episode: 163, duration: 0.668s, episode steps: 40, steps per second: 60, episode reward: 40.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.575 [0.000, 1.000], mean observation: 0.047 [-1.678, 1.361], loss: 1.273061, mean_absolute_error: 4.325058, mean_q: 8.029399\n",
      " 1883/5000: episode: 164, duration: 0.501s, episode steps: 30, steps per second: 60, episode reward: 30.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.533 [0.000, 1.000], mean observation: -0.069 [-1.217, 0.564], loss: 1.406929, mean_absolute_error: 4.391330, mean_q: 8.255497\n",
      " 1893/5000: episode: 165, duration: 0.164s, episode steps: 10, steps per second: 61, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.800 [0.000, 1.000], mean observation: -0.109 [-1.891, 1.189], loss: 1.541098, mean_absolute_error: 4.498346, mean_q: 8.395944\n",
      " 1905/5000: episode: 166, duration: 0.199s, episode steps: 12, steps per second: 60, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.750 [0.000, 1.000], mean observation: -0.111 [-1.970, 1.195], loss: 1.308313, mean_absolute_error: 4.561944, mean_q: 8.538512\n",
      " 1919/5000: episode: 167, duration: 0.232s, episode steps: 14, steps per second: 60, episode reward: 14.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.571 [0.000, 1.000], mean observation: -0.124 [-1.260, 0.587], loss: 1.598802, mean_absolute_error: 4.565463, mean_q: 8.464767\n",
      " 1930/5000: episode: 168, duration: 0.190s, episode steps: 11, steps per second: 58, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.727 [0.000, 1.000], mean observation: -0.119 [-1.948, 1.198], loss: 1.479879, mean_absolute_error: 4.563978, mean_q: 8.481631\n",
      " 1945/5000: episode: 169, duration: 0.243s, episode steps: 15, steps per second: 62, episode reward: 15.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.533 [0.000, 1.000], mean observation: -0.075 [-1.280, 0.832], loss: 1.576853, mean_absolute_error: 4.559260, mean_q: 8.495253\n",
      " 2012/5000: episode: 170, duration: 1.115s, episode steps: 67, steps per second: 60, episode reward: 67.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.552 [0.000, 1.000], mean observation: 0.065 [-1.453, 1.498], loss: 2.228511, mean_absolute_error: 4.633653, mean_q: 8.505608\n",
      " 2054/5000: episode: 171, duration: 0.695s, episode steps: 42, steps per second: 60, episode reward: 42.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.452 [0.000, 1.000], mean observation: -0.123 [-0.774, 0.160], loss: 1.814195, mean_absolute_error: 4.698818, mean_q: 8.662631\n",
      " 2066/5000: episode: 172, duration: 0.203s, episode steps: 12, steps per second: 59, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.750 [0.000, 1.000], mean observation: -0.082 [-1.888, 1.202], loss: 1.189746, mean_absolute_error: 4.664857, mean_q: 8.731737\n",
      " 2077/5000: episode: 173, duration: 0.181s, episode steps: 11, steps per second: 61, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.727 [0.000, 1.000], mean observation: -0.105 [-1.931, 1.211], loss: 2.582877, mean_absolute_error: 4.714543, mean_q: 8.657291\n",
      " 2089/5000: episode: 174, duration: 0.199s, episode steps: 12, steps per second: 60, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.750 [0.000, 1.000], mean observation: -0.096 [-1.868, 1.142], loss: 1.242381, mean_absolute_error: 4.700987, mean_q: 8.818055\n",
      " 2100/5000: episode: 175, duration: 0.182s, episode steps: 11, steps per second: 60, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.727 [0.000, 1.000], mean observation: -0.113 [-1.908, 1.191], loss: 2.040995, mean_absolute_error: 5.103509, mean_q: 9.502639\n",
      " 2113/5000: episode: 176, duration: 0.214s, episode steps: 13, steps per second: 61, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.769 [0.000, 1.000], mean observation: -0.108 [-2.185, 1.335], loss: 2.104454, mean_absolute_error: 4.963093, mean_q: 9.260024\n",
      " 2123/5000: episode: 177, duration: 0.169s, episode steps: 10, steps per second: 59, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.900 [0.000, 1.000], mean observation: -0.111 [-2.424, 1.613], loss: 1.672554, mean_absolute_error: 4.858346, mean_q: 9.063303\n",
      " 2133/5000: episode: 178, duration: 0.166s, episode steps: 10, steps per second: 60, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.800 [0.000, 1.000], mean observation: -0.133 [-2.235, 1.352], loss: 1.322760, mean_absolute_error: 4.975843, mean_q: 9.373745\n",
      " 2144/5000: episode: 179, duration: 0.183s, episode steps: 11, steps per second: 60, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.727 [0.000, 1.000], mean observation: -0.118 [-1.808, 1.212], loss: 2.320419, mean_absolute_error: 5.109953, mean_q: 9.547186\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 2152/5000: episode: 180, duration: 0.132s, episode steps: 8, steps per second: 61, episode reward: 8.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.875 [0.000, 1.000], mean observation: -0.123 [-2.180, 1.399], loss: 2.619481, mean_absolute_error: 5.083424, mean_q: 9.345673\n",
      " 2163/5000: episode: 181, duration: 0.183s, episode steps: 11, steps per second: 60, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.727 [0.000, 1.000], mean observation: -0.106 [-1.927, 1.211], loss: 1.302474, mean_absolute_error: 4.941520, mean_q: 9.228623\n",
      " 2175/5000: episode: 182, duration: 0.196s, episode steps: 12, steps per second: 61, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.750 [0.000, 1.000], mean observation: -0.116 [-2.163, 1.363], loss: 1.888739, mean_absolute_error: 5.087807, mean_q: 9.409200\n",
      " 2185/5000: episode: 183, duration: 0.168s, episode steps: 10, steps per second: 60, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.800 [0.000, 1.000], mean observation: -0.143 [-2.189, 1.339], loss: 1.566859, mean_absolute_error: 5.009350, mean_q: 9.355615\n",
      " 2194/5000: episode: 184, duration: 0.149s, episode steps: 9, steps per second: 60, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.889 [0.000, 1.000], mean observation: -0.154 [-2.477, 1.549], loss: 2.132683, mean_absolute_error: 5.196315, mean_q: 9.583288\n",
      " 2202/5000: episode: 185, duration: 0.134s, episode steps: 8, steps per second: 60, episode reward: 8.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.165 [-2.556, 1.523], loss: 1.396659, mean_absolute_error: 5.137375, mean_q: 9.716223\n",
      " 2211/5000: episode: 186, duration: 0.151s, episode steps: 9, steps per second: 60, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.889 [0.000, 1.000], mean observation: -0.149 [-2.325, 1.370], loss: 2.759987, mean_absolute_error: 5.231871, mean_q: 9.757445\n",
      " 2224/5000: episode: 187, duration: 0.216s, episode steps: 13, steps per second: 60, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.769 [0.000, 1.000], mean observation: -0.115 [-2.213, 1.325], loss: 1.683141, mean_absolute_error: 5.228847, mean_q: 9.772298\n",
      " 2233/5000: episode: 188, duration: 0.144s, episode steps: 9, steps per second: 63, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.778 [0.000, 1.000], mean observation: -0.111 [-1.907, 1.197], loss: 2.253471, mean_absolute_error: 5.207842, mean_q: 9.644580\n",
      " 2244/5000: episode: 189, duration: 0.186s, episode steps: 11, steps per second: 59, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.909 [0.000, 1.000], mean observation: -0.130 [-2.796, 1.804], loss: 2.219854, mean_absolute_error: 5.295286, mean_q: 9.746030\n",
      " 2257/5000: episode: 190, duration: 0.216s, episode steps: 13, steps per second: 60, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.769 [0.000, 1.000], mean observation: -0.110 [-2.432, 1.536], loss: 2.788244, mean_absolute_error: 5.256271, mean_q: 9.562219\n",
      " 2268/5000: episode: 191, duration: 0.182s, episode steps: 11, steps per second: 61, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.818 [0.000, 1.000], mean observation: -0.152 [-2.246, 1.335], loss: 1.627008, mean_absolute_error: 5.297038, mean_q: 9.871047\n",
      " 2279/5000: episode: 192, duration: 0.182s, episode steps: 11, steps per second: 60, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.909 [0.000, 1.000], mean observation: -0.122 [-2.829, 1.804], loss: 1.480380, mean_absolute_error: 5.290284, mean_q: 9.961399\n",
      " 2289/5000: episode: 193, duration: 0.165s, episode steps: 10, steps per second: 60, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.900 [0.000, 1.000], mean observation: -0.150 [-2.757, 1.763], loss: 2.105970, mean_absolute_error: 5.279832, mean_q: 9.814189\n",
      " 2297/5000: episode: 194, duration: 0.129s, episode steps: 8, steps per second: 62, episode reward: 8.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.750 [0.000, 1.000], mean observation: -0.158 [-1.902, 1.131], loss: 2.529914, mean_absolute_error: 5.481781, mean_q: 10.148699\n",
      " 2309/5000: episode: 195, duration: 0.202s, episode steps: 12, steps per second: 59, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.667 [0.000, 1.000], mean observation: -0.119 [-1.474, 0.787], loss: 2.576542, mean_absolute_error: 5.331335, mean_q: 9.823934\n",
      " 2321/5000: episode: 196, duration: 0.199s, episode steps: 12, steps per second: 60, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.667 [0.000, 1.000], mean observation: -0.100 [-1.519, 0.830], loss: 1.521572, mean_absolute_error: 5.316402, mean_q: 9.971302\n",
      " 2331/5000: episode: 197, duration: 0.164s, episode steps: 10, steps per second: 61, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.800 [0.000, 1.000], mean observation: -0.132 [-2.093, 1.204], loss: 1.861654, mean_absolute_error: 5.252151, mean_q: 9.783495\n",
      " 2342/5000: episode: 198, duration: 0.184s, episode steps: 11, steps per second: 60, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.727 [0.000, 1.000], mean observation: -0.125 [-1.885, 1.158], loss: 2.576833, mean_absolute_error: 5.403160, mean_q: 9.996723\n",
      " 2352/5000: episode: 199, duration: 0.167s, episode steps: 10, steps per second: 60, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.800 [0.000, 1.000], mean observation: -0.111 [-2.202, 1.354], loss: 1.959687, mean_absolute_error: 5.277911, mean_q: 9.825672\n",
      " 2362/5000: episode: 200, duration: 0.169s, episode steps: 10, steps per second: 59, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.900 [0.000, 1.000], mean observation: -0.124 [-2.743, 1.806], loss: 3.862037, mean_absolute_error: 5.595148, mean_q: 10.106374\n",
      " 2375/5000: episode: 201, duration: 0.210s, episode steps: 13, steps per second: 62, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.769 [0.000, 1.000], mean observation: -0.078 [-2.221, 1.412], loss: 2.384098, mean_absolute_error: 5.433314, mean_q: 9.996537\n",
      " 2384/5000: episode: 202, duration: 0.150s, episode steps: 9, steps per second: 60, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.145 [-2.777, 1.721], loss: 2.197908, mean_absolute_error: 5.468793, mean_q: 10.050246\n",
      " 2395/5000: episode: 203, duration: 0.183s, episode steps: 11, steps per second: 60, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.909 [0.000, 1.000], mean observation: -0.135 [-2.751, 1.724], loss: 2.365191, mean_absolute_error: 5.565175, mean_q: 10.254950\n",
      " 2404/5000: episode: 204, duration: 0.149s, episode steps: 9, steps per second: 60, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.136 [-2.812, 1.764], loss: 1.411737, mean_absolute_error: 5.401979, mean_q: 10.085526\n",
      " 2414/5000: episode: 205, duration: 0.163s, episode steps: 10, steps per second: 61, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.101 [-2.973, 2.000], loss: 1.369691, mean_absolute_error: 5.288830, mean_q: 9.925312\n",
      " 2424/5000: episode: 206, duration: 0.170s, episode steps: 10, steps per second: 59, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.900 [0.000, 1.000], mean observation: -0.126 [-2.702, 1.801], loss: 3.156756, mean_absolute_error: 5.501414, mean_q: 10.153708\n",
      " 2436/5000: episode: 207, duration: 0.199s, episode steps: 12, steps per second: 60, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.750 [0.000, 1.000], mean observation: -0.120 [-2.098, 1.380], loss: 2.850572, mean_absolute_error: 5.463456, mean_q: 10.058945\n",
      " 2445/5000: episode: 208, duration: 0.149s, episode steps: 9, steps per second: 60, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.778 [0.000, 1.000], mean observation: -0.135 [-1.948, 1.184], loss: 1.808443, mean_absolute_error: 5.492755, mean_q: 10.158158\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 2456/5000: episode: 209, duration: 0.182s, episode steps: 11, steps per second: 60, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.818 [0.000, 1.000], mean observation: -0.108 [-2.402, 1.607], loss: 2.420222, mean_absolute_error: 5.579640, mean_q: 10.234366\n",
      " 2465/5000: episode: 210, duration: 0.149s, episode steps: 9, steps per second: 60, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.889 [0.000, 1.000], mean observation: -0.161 [-2.304, 1.357], loss: 2.065156, mean_absolute_error: 5.564383, mean_q: 10.305596\n",
      " 2475/5000: episode: 211, duration: 0.162s, episode steps: 10, steps per second: 62, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.900 [0.000, 1.000], mean observation: -0.136 [-2.758, 1.781], loss: 1.763359, mean_absolute_error: 5.551276, mean_q: 10.389808\n",
      " 2485/5000: episode: 212, duration: 0.169s, episode steps: 10, steps per second: 59, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.900 [0.000, 1.000], mean observation: -0.137 [-2.714, 1.745], loss: 2.853166, mean_absolute_error: 5.645645, mean_q: 10.437580\n",
      " 2495/5000: episode: 213, duration: 0.166s, episode steps: 10, steps per second: 60, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.800 [0.000, 1.000], mean observation: -0.132 [-2.086, 1.375], loss: 2.378803, mean_absolute_error: 5.540604, mean_q: 10.237546\n",
      " 2505/5000: episode: 214, duration: 0.165s, episode steps: 10, steps per second: 60, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.800 [0.000, 1.000], mean observation: -0.133 [-2.027, 1.172], loss: 2.121299, mean_absolute_error: 5.533391, mean_q: 10.390615\n",
      " 2516/5000: episode: 215, duration: 0.183s, episode steps: 11, steps per second: 60, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.909 [0.000, 1.000], mean observation: -0.134 [-2.838, 1.762], loss: 2.412621, mean_absolute_error: 5.664299, mean_q: 10.552155\n",
      " 2526/5000: episode: 216, duration: 0.165s, episode steps: 10, steps per second: 61, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.120 [-2.985, 1.956], loss: 1.402773, mean_absolute_error: 5.540192, mean_q: 10.426943\n",
      " 2535/5000: episode: 217, duration: 0.147s, episode steps: 9, steps per second: 61, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.148 [-2.844, 1.782], loss: 2.451357, mean_absolute_error: 5.591051, mean_q: 10.405659\n",
      " 2545/5000: episode: 218, duration: 0.170s, episode steps: 10, steps per second: 59, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.157 [-3.057, 1.915], loss: 2.717339, mean_absolute_error: 5.676900, mean_q: 10.496750\n",
      " 2555/5000: episode: 219, duration: 0.166s, episode steps: 10, steps per second: 60, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.157 [-3.119, 1.934], loss: 2.037874, mean_absolute_error: 5.518533, mean_q: 10.327246\n",
      " 2563/5000: episode: 220, duration: 0.132s, episode steps: 8, steps per second: 61, episode reward: 8.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.875 [0.000, 1.000], mean observation: -0.133 [-2.220, 1.394], loss: 2.144372, mean_absolute_error: 5.641040, mean_q: 10.501657\n",
      " 2572/5000: episode: 221, duration: 0.149s, episode steps: 9, steps per second: 60, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.146 [-2.831, 1.807], loss: 2.213170, mean_absolute_error: 5.667381, mean_q: 10.569418\n",
      " 2581/5000: episode: 222, duration: 0.150s, episode steps: 9, steps per second: 60, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.149 [-2.769, 1.747], loss: 1.564947, mean_absolute_error: 5.660647, mean_q: 10.748005\n",
      " 2592/5000: episode: 223, duration: 0.180s, episode steps: 11, steps per second: 61, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.909 [0.000, 1.000], mean observation: -0.139 [-2.868, 1.770], loss: 2.370343, mean_absolute_error: 5.708581, mean_q: 10.765537\n",
      " 2601/5000: episode: 224, duration: 0.148s, episode steps: 9, steps per second: 61, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.144 [-2.832, 1.749], loss: 1.753725, mean_absolute_error: 5.698568, mean_q: 10.735997\n",
      " 2610/5000: episode: 225, duration: 0.151s, episode steps: 9, steps per second: 60, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.163 [-2.794, 1.719], loss: 3.482129, mean_absolute_error: 5.971371, mean_q: 10.948251\n",
      " 2619/5000: episode: 226, duration: 0.149s, episode steps: 9, steps per second: 60, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.149 [-2.830, 1.738], loss: 2.411723, mean_absolute_error: 5.668159, mean_q: 10.478627\n",
      " 2628/5000: episode: 227, duration: 0.150s, episode steps: 9, steps per second: 60, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.135 [-2.811, 1.794], loss: 2.376637, mean_absolute_error: 5.811606, mean_q: 10.735065\n",
      " 2637/5000: episode: 228, duration: 0.150s, episode steps: 9, steps per second: 60, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.889 [0.000, 1.000], mean observation: -0.152 [-2.271, 1.363], loss: 3.064509, mean_absolute_error: 5.798891, mean_q: 10.540555\n",
      " 2645/5000: episode: 229, duration: 0.133s, episode steps: 8, steps per second: 60, episode reward: 8.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.139 [-2.572, 1.582], loss: 2.850933, mean_absolute_error: 5.812924, mean_q: 10.598511\n",
      " 2655/5000: episode: 230, duration: 0.163s, episode steps: 10, steps per second: 61, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.800 [0.000, 1.000], mean observation: -0.142 [-1.983, 1.148], loss: 1.640751, mean_absolute_error: 5.597593, mean_q: 10.341899\n",
      " 2667/5000: episode: 231, duration: 0.203s, episode steps: 12, steps per second: 59, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.750 [0.000, 1.000], mean observation: -0.106 [-1.953, 1.187], loss: 2.468436, mean_absolute_error: 5.616552, mean_q: 10.258833\n",
      " 2677/5000: episode: 232, duration: 0.163s, episode steps: 10, steps per second: 61, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.800 [0.000, 1.000], mean observation: -0.113 [-1.977, 1.189], loss: 2.010147, mean_absolute_error: 5.597360, mean_q: 10.392530\n",
      " 2687/5000: episode: 233, duration: 0.170s, episode steps: 10, steps per second: 59, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.900 [0.000, 1.000], mean observation: -0.156 [-2.568, 1.519], loss: 2.598999, mean_absolute_error: 5.761550, mean_q: 10.575445\n",
      " 2696/5000: episode: 234, duration: 0.149s, episode steps: 9, steps per second: 60, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.889 [0.000, 1.000], mean observation: -0.135 [-2.486, 1.611], loss: 1.927624, mean_absolute_error: 5.614469, mean_q: 10.410636\n",
      " 2706/5000: episode: 235, duration: 0.165s, episode steps: 10, steps per second: 61, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.145 [-3.086, 1.927], loss: 1.636155, mean_absolute_error: 5.614463, mean_q: 10.547374\n",
      " 2714/5000: episode: 236, duration: 0.128s, episode steps: 8, steps per second: 63, episode reward: 8.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.136 [-2.564, 1.585], loss: 1.614424, mean_absolute_error: 5.514649, mean_q: 10.245935\n",
      " 2724/5000: episode: 237, duration: 0.168s, episode steps: 10, steps per second: 60, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.108 [-2.967, 1.988], loss: 2.001435, mean_absolute_error: 5.657191, mean_q: 10.476976\n",
      " 2733/5000: episode: 238, duration: 0.148s, episode steps: 9, steps per second: 61, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.889 [0.000, 1.000], mean observation: -0.108 [-2.417, 1.609], loss: 1.867705, mean_absolute_error: 5.718342, mean_q: 10.650616\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 2746/5000: episode: 239, duration: 0.218s, episode steps: 13, steps per second: 60, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.538 [0.000, 1.000], mean observation: -0.114 [-1.166, 0.616], loss: 2.106350, mean_absolute_error: 5.723833, mean_q: 10.638388\n",
      " 2756/5000: episode: 240, duration: 0.167s, episode steps: 10, steps per second: 60, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.700 [0.000, 1.000], mean observation: -0.115 [-1.444, 0.775], loss: 2.603588, mean_absolute_error: 5.773190, mean_q: 10.624571\n",
      " 2766/5000: episode: 241, duration: 0.165s, episode steps: 10, steps per second: 60, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.800 [0.000, 1.000], mean observation: -0.100 [-1.945, 1.222], loss: 1.717933, mean_absolute_error: 5.588039, mean_q: 10.463107\n",
      " 2775/5000: episode: 242, duration: 0.145s, episode steps: 9, steps per second: 62, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.778 [0.000, 1.000], mean observation: -0.133 [-1.845, 1.158], loss: 2.171665, mean_absolute_error: 5.743769, mean_q: 10.650889\n",
      " 2786/5000: episode: 243, duration: 0.185s, episode steps: 11, steps per second: 59, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.636 [0.000, 1.000], mean observation: -0.127 [-1.415, 0.790], loss: 2.060164, mean_absolute_error: 5.681439, mean_q: 10.624523\n",
      " 2797/5000: episode: 244, duration: 0.182s, episode steps: 11, steps per second: 61, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.818 [0.000, 1.000], mean observation: -0.122 [-2.214, 1.366], loss: 1.452142, mean_absolute_error: 5.728131, mean_q: 10.723540\n",
      " 2809/5000: episode: 245, duration: 0.202s, episode steps: 12, steps per second: 59, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.750 [0.000, 1.000], mean observation: -0.077 [-1.887, 1.197], loss: 2.059591, mean_absolute_error: 5.705597, mean_q: 10.576974\n",
      " 2891/5000: episode: 246, duration: 1.368s, episode steps: 82, steps per second: 60, episode reward: 82.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.598 [0.000, 1.000], mean observation: 0.033 [-3.353, 3.101], loss: 2.300493, mean_absolute_error: 5.689871, mean_q: 10.458141\n",
      " 2903/5000: episode: 247, duration: 0.196s, episode steps: 12, steps per second: 61, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.667 [0.000, 1.000], mean observation: -0.122 [-1.700, 0.947], loss: 1.641808, mean_absolute_error: 5.570843, mean_q: 10.284311\n",
      " 2912/5000: episode: 248, duration: 0.149s, episode steps: 9, steps per second: 60, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.889 [0.000, 1.000], mean observation: -0.130 [-2.443, 1.542], loss: 1.876898, mean_absolute_error: 5.691085, mean_q: 10.627206\n",
      " 2923/5000: episode: 249, duration: 0.182s, episode steps: 11, steps per second: 60, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.636 [0.000, 1.000], mean observation: -0.109 [-1.426, 0.833], loss: 2.313444, mean_absolute_error: 5.544534, mean_q: 10.269683\n",
      " 2935/5000: episode: 250, duration: 0.196s, episode steps: 12, steps per second: 61, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.667 [0.000, 1.000], mean observation: -0.117 [-1.478, 0.754], loss: 1.556116, mean_absolute_error: 5.709375, mean_q: 10.686231\n",
      " 2950/5000: episode: 251, duration: 0.252s, episode steps: 15, steps per second: 59, episode reward: 15.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.800 [0.000, 1.000], mean observation: -0.050 [-2.670, 1.801], loss: 2.120486, mean_absolute_error: 5.680771, mean_q: 10.570492\n",
      " 2961/5000: episode: 252, duration: 0.178s, episode steps: 11, steps per second: 62, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.818 [0.000, 1.000], mean observation: -0.100 [-2.393, 1.589], loss: 1.451539, mean_absolute_error: 5.747732, mean_q: 10.705832\n",
      " 2973/5000: episode: 253, duration: 0.203s, episode steps: 12, steps per second: 59, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.667 [0.000, 1.000], mean observation: -0.126 [-1.677, 0.935], loss: 2.216292, mean_absolute_error: 5.778429, mean_q: 10.779742\n",
      " 2987/5000: episode: 254, duration: 0.233s, episode steps: 14, steps per second: 60, episode reward: 14.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.571 [0.000, 1.000], mean observation: -0.097 [-1.236, 0.778], loss: 1.898726, mean_absolute_error: 5.673754, mean_q: 10.567454\n",
      " 3003/5000: episode: 255, duration: 0.265s, episode steps: 16, steps per second: 60, episode reward: 16.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.081 [-0.997, 0.589], loss: 1.589287, mean_absolute_error: 5.657176, mean_q: 10.561225\n",
      " 3053/5000: episode: 256, duration: 0.832s, episode steps: 50, steps per second: 60, episode reward: 50.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.660 [0.000, 1.000], mean observation: 0.066 [-3.453, 3.012], loss: 1.754025, mean_absolute_error: 5.663198, mean_q: 10.552208\n",
      " 3082/5000: episode: 257, duration: 0.484s, episode steps: 29, steps per second: 60, episode reward: 29.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.552 [0.000, 1.000], mean observation: 0.110 [-0.205, 0.578], loss: 1.886921, mean_absolute_error: 5.707471, mean_q: 10.583606\n",
      " 3104/5000: episode: 258, duration: 0.363s, episode steps: 22, steps per second: 61, episode reward: 22.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.079 [-0.430, 0.791], loss: 1.618692, mean_absolute_error: 5.610222, mean_q: 10.422036\n",
      " 3133/5000: episode: 259, duration: 0.482s, episode steps: 29, steps per second: 60, episode reward: 29.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.517 [0.000, 1.000], mean observation: 0.069 [-0.573, 0.849], loss: 1.619899, mean_absolute_error: 5.596114, mean_q: 10.409338\n",
      " 3162/5000: episode: 260, duration: 0.484s, episode steps: 29, steps per second: 60, episode reward: 29.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.552 [0.000, 1.000], mean observation: 0.115 [-0.388, 0.676], loss: 1.696155, mean_absolute_error: 5.696192, mean_q: 10.565962\n",
      " 3208/5000: episode: 261, duration: 0.768s, episode steps: 46, steps per second: 60, episode reward: 46.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.652 [0.000, 1.000], mean observation: 0.115 [-3.057, 2.729], loss: 1.803499, mean_absolute_error: 5.702266, mean_q: 10.581492\n",
      " 3273/5000: episode: 262, duration: 1.081s, episode steps: 65, steps per second: 60, episode reward: 65.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.523 [0.000, 1.000], mean observation: 0.037 [-0.357, 0.589], loss: 1.549713, mean_absolute_error: 5.740747, mean_q: 10.723129\n",
      " 3338/5000: episode: 263, duration: 1.079s, episode steps: 65, steps per second: 60, episode reward: 65.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.615 [0.000, 1.000], mean observation: 0.108 [-3.024, 2.859], loss: 1.455637, mean_absolute_error: 5.727771, mean_q: 10.700202\n",
      " 3386/5000: episode: 264, duration: 0.805s, episode steps: 48, steps per second: 60, episode reward: 48.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.542 [0.000, 1.000], mean observation: 0.016 [-1.180, 0.924], loss: 1.538389, mean_absolute_error: 5.799674, mean_q: 10.851216\n",
      " 3457/5000: episode: 265, duration: 1.182s, episode steps: 71, steps per second: 60, episode reward: 71.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.577 [0.000, 1.000], mean observation: 0.097 [-2.205, 2.067], loss: 1.366010, mean_absolute_error: 5.910888, mean_q: 11.104767\n",
      " 3484/5000: episode: 266, duration: 0.447s, episode steps: 27, steps per second: 60, episode reward: 27.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.481 [0.000, 1.000], mean observation: 0.051 [-0.646, 1.380], loss: 1.432679, mean_absolute_error: 5.923586, mean_q: 11.123781\n",
      " 3580/5000: episode: 267, duration: 1.598s, episode steps: 96, steps per second: 60, episode reward: 96.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.604 [0.000, 1.000], mean observation: 0.138 [-3.756, 3.853], loss: 1.332506, mean_absolute_error: 6.072355, mean_q: 11.443311\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 3631/5000: episode: 268, duration: 0.849s, episode steps: 51, steps per second: 60, episode reward: 51.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.490 [0.000, 1.000], mean observation: 0.007 [-0.578, 0.970], loss: 1.691353, mean_absolute_error: 6.109680, mean_q: 11.413475\n",
      " 3691/5000: episode: 269, duration: 1.000s, episode steps: 60, steps per second: 60, episode reward: 60.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.633 [0.000, 1.000], mean observation: 0.173 [-2.984, 3.210], loss: 1.754368, mean_absolute_error: 6.182782, mean_q: 11.554001\n",
      " 3743/5000: episode: 270, duration: 0.864s, episode steps: 52, steps per second: 60, episode reward: 52.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.519 [0.000, 1.000], mean observation: 0.048 [-0.411, 0.857], loss: 1.563946, mean_absolute_error: 6.227271, mean_q: 11.672683\n",
      " 3801/5000: episode: 271, duration: 0.967s, episode steps: 58, steps per second: 60, episode reward: 58.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.009 [-0.622, 0.930], loss: 1.853753, mean_absolute_error: 6.215734, mean_q: 11.582305\n",
      " 3823/5000: episode: 272, duration: 0.364s, episode steps: 22, steps per second: 60, episode reward: 22.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.545 [0.000, 1.000], mean observation: 0.113 [-0.414, 0.938], loss: 1.553306, mean_absolute_error: 6.275906, mean_q: 11.736072\n",
      " 3899/5000: episode: 273, duration: 1.263s, episode steps: 76, steps per second: 60, episode reward: 76.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.618 [0.000, 1.000], mean observation: 0.211 [-3.159, 3.455], loss: 1.496181, mean_absolute_error: 6.357324, mean_q: 11.953170\n",
      " 3933/5000: episode: 274, duration: 0.569s, episode steps: 34, steps per second: 60, episode reward: 34.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.529 [0.000, 1.000], mean observation: 0.090 [-0.237, 0.649], loss: 1.647555, mean_absolute_error: 6.401426, mean_q: 11.964786\n",
      " 3969/5000: episode: 275, duration: 0.600s, episode steps: 36, steps per second: 60, episode reward: 36.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.528 [0.000, 1.000], mean observation: 0.099 [-0.385, 0.641], loss: 1.432678, mean_absolute_error: 6.426157, mean_q: 12.097927\n",
      " 4013/5000: episode: 276, duration: 0.733s, episode steps: 44, steps per second: 60, episode reward: 44.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.523 [0.000, 1.000], mean observation: 0.076 [-0.347, 0.627], loss: 1.739005, mean_absolute_error: 6.484688, mean_q: 12.104633\n",
      " 4050/5000: episode: 277, duration: 0.614s, episode steps: 37, steps per second: 60, episode reward: 37.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.541 [0.000, 1.000], mean observation: 0.142 [-0.168, 0.765], loss: 1.596309, mean_absolute_error: 6.599085, mean_q: 12.407540\n",
      " 4116/5000: episode: 278, duration: 1.097s, episode steps: 66, steps per second: 60, episode reward: 66.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.530 [0.000, 1.000], mean observation: 0.072 [-0.417, 0.696], loss: 1.709566, mean_absolute_error: 6.558799, mean_q: 12.298192\n",
      " 4191/5000: episode: 279, duration: 1.249s, episode steps: 75, steps per second: 60, episode reward: 75.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.533 [0.000, 1.000], mean observation: 0.083 [-0.276, 0.959], loss: 1.648904, mean_absolute_error: 6.632578, mean_q: 12.475681\n",
      " 4240/5000: episode: 280, duration: 0.817s, episode steps: 49, steps per second: 60, episode reward: 49.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.531 [0.000, 1.000], mean observation: 0.092 [-0.241, 0.689], loss: 1.695614, mean_absolute_error: 6.755756, mean_q: 12.654406\n",
      " 4382/5000: episode: 281, duration: 2.365s, episode steps: 142, steps per second: 60, episode reward: 142.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.514 [0.000, 1.000], mean observation: 0.009 [-0.623, 0.706], loss: 1.672428, mean_absolute_error: 6.845905, mean_q: 12.888423\n",
      " 4466/5000: episode: 282, duration: 1.400s, episode steps: 84, steps per second: 60, episode reward: 84.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.536 [0.000, 1.000], mean observation: 0.136 [-0.317, 1.057], loss: 1.531188, mean_absolute_error: 7.035250, mean_q: 13.325261\n",
      " 4504/5000: episode: 283, duration: 0.631s, episode steps: 38, steps per second: 60, episode reward: 38.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.526 [0.000, 1.000], mean observation: 0.050 [-0.434, 0.777], loss: 2.093228, mean_absolute_error: 7.158877, mean_q: 13.424289\n",
      " 4534/5000: episode: 284, duration: 0.500s, episode steps: 30, steps per second: 60, episode reward: 30.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.533 [0.000, 1.000], mean observation: 0.078 [-0.397, 0.732], loss: 1.874686, mean_absolute_error: 7.201632, mean_q: 13.517749\n",
      " 4575/5000: episode: 285, duration: 0.677s, episode steps: 41, steps per second: 61, episode reward: 41.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.537 [0.000, 1.000], mean observation: 0.112 [-0.538, 0.870], loss: 1.756295, mean_absolute_error: 7.223278, mean_q: 13.645153\n",
      " 4615/5000: episode: 286, duration: 0.668s, episode steps: 40, steps per second: 60, episode reward: 40.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.525 [0.000, 1.000], mean observation: 0.095 [-0.667, 0.970], loss: 2.345099, mean_absolute_error: 7.342468, mean_q: 13.771749\n",
      " 4663/5000: episode: 287, duration: 0.799s, episode steps: 48, steps per second: 60, episode reward: 48.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.542 [0.000, 1.000], mean observation: 0.142 [-0.347, 0.718], loss: 1.565128, mean_absolute_error: 7.348362, mean_q: 13.905357\n",
      " 4724/5000: episode: 288, duration: 1.016s, episode steps: 61, steps per second: 60, episode reward: 61.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.508 [0.000, 1.000], mean observation: 0.004 [-0.556, 0.872], loss: 2.290610, mean_absolute_error: 7.396508, mean_q: 13.857903\n",
      " 4746/5000: episode: 289, duration: 0.364s, episode steps: 22, steps per second: 60, episode reward: 22.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.545 [0.000, 1.000], mean observation: 0.098 [-0.432, 0.874], loss: 1.493567, mean_absolute_error: 7.403107, mean_q: 13.981241\n",
      " 4811/5000: episode: 290, duration: 1.084s, episode steps: 65, steps per second: 60, episode reward: 65.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.523 [0.000, 1.000], mean observation: 0.060 [-0.406, 0.722], loss: 2.021109, mean_absolute_error: 7.528187, mean_q: 14.177201\n",
      " 4902/5000: episode: 291, duration: 1.515s, episode steps: 91, steps per second: 60, episode reward: 91.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.516 [0.000, 1.000], mean observation: -0.042 [-0.597, 0.662], loss: 1.959881, mean_absolute_error: 7.568567, mean_q: 14.316430\n",
      " 4947/5000: episode: 292, duration: 0.748s, episode steps: 45, steps per second: 60, episode reward: 45.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.444 [0.000, 1.000], mean observation: -0.117 [-0.878, 0.237], loss: 2.354646, mean_absolute_error: 7.763598, mean_q: 14.620833\n",
      " 4971/5000: episode: 293, duration: 0.399s, episode steps: 24, steps per second: 60, episode reward: 24.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.114 [-0.351, 0.761], loss: 2.296712, mean_absolute_error: 7.731984, mean_q: 14.515830\n",
      "done, took 83.942 seconds\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1ebf820fb70>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Training with verbose and visualization\n",
    "dqn.fit(env, nb_steps=5000, visualize=True, verbose=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing for 20 episodes ...\n",
      "Episode 1: reward: 42.000, steps: 42\n",
      "Episode 2: reward: 62.000, steps: 62\n",
      "Episode 3: reward: 31.000, steps: 31\n",
      "Episode 4: reward: 51.000, steps: 51\n",
      "Episode 5: reward: 46.000, steps: 46\n",
      "Episode 6: reward: 36.000, steps: 36\n",
      "Episode 7: reward: 35.000, steps: 35\n",
      "Episode 8: reward: 63.000, steps: 63\n",
      "Episode 9: reward: 40.000, steps: 40\n",
      "Episode 10: reward: 44.000, steps: 44\n",
      "Episode 11: reward: 66.000, steps: 66\n",
      "Episode 12: reward: 44.000, steps: 44\n",
      "Episode 13: reward: 32.000, steps: 32\n",
      "Episode 14: reward: 44.000, steps: 44\n",
      "Episode 15: reward: 36.000, steps: 36\n",
      "Episode 16: reward: 30.000, steps: 30\n",
      "Episode 17: reward: 43.000, steps: 43\n",
      "Episode 18: reward: 47.000, steps: 47\n",
      "Episode 19: reward: 35.000, steps: 35\n",
      "Episode 20: reward: 79.000, steps: 79\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1eb8a2774e0>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test the model\n",
    "dqn.test(env, nb_episodes=20, visualize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Train and Test a DQN (no verbosity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for 100000 steps ...\n",
      "Interval 1 (0 steps performed)\n",
      "10000/10000 [==============================] - 19s 2ms/step - reward: 1.0000\n",
      "105 episodes - episode_reward: 94.790 [29.000, 200.000] - loss: 5.346 - mean_absolute_error: 14.495 - mean_q: 28.363\n",
      "\n",
      "Interval 2 (10000 steps performed)\n",
      "10000/10000 [==============================] - 19s 2ms/step - reward: 1.0000\n",
      "51 episodes - episode_reward: 194.922 [88.000, 200.000] - loss: 12.340 - mean_absolute_error: 27.364 - mean_q: 55.054\n",
      "\n",
      "Interval 3 (20000 steps performed)\n",
      "10000/10000 [==============================] - 19s 2ms/step - reward: 1.0000\n",
      "50 episodes - episode_reward: 200.000 [200.000, 200.000] - loss: 18.884 - mean_absolute_error: 37.948 - mean_q: 76.908\n",
      "\n",
      "Interval 4 (30000 steps performed)\n",
      "10000/10000 [==============================] - 19s 2ms/step - reward: 1.0000\n",
      "50 episodes - episode_reward: 200.000 [200.000, 200.000] - loss: 22.428 - mean_absolute_error: 43.282 - mean_q: 87.853\n",
      "\n",
      "Interval 5 (40000 steps performed)\n",
      "10000/10000 [==============================] - 19s 2ms/step - reward: 1.0000\n",
      "50 episodes - episode_reward: 200.000 [200.000, 200.000] - loss: 23.853 - mean_absolute_error: 44.914 - mean_q: 91.326\n",
      "\n",
      "Interval 6 (50000 steps performed)\n",
      "10000/10000 [==============================] - 19s 2ms/step - reward: 1.0000\n",
      "50 episodes - episode_reward: 200.000 [200.000, 200.000] - loss: 19.451 - mean_absolute_error: 44.271 - mean_q: 89.980\n",
      "\n",
      "Interval 7 (60000 steps performed)\n",
      "10000/10000 [==============================] - 19s 2ms/step - reward: 1.0000\n",
      "50 episodes - episode_reward: 200.000 [200.000, 200.000] - loss: 15.757 - mean_absolute_error: 40.543 - mean_q: 81.668\n",
      "\n",
      "Interval 8 (70000 steps performed)\n",
      "10000/10000 [==============================] - 19s 2ms/step - reward: 1.0000\n",
      "52 episodes - episode_reward: 192.981 [17.000, 200.000] - loss: 13.780 - mean_absolute_error: 38.232 - mean_q: 76.504\n",
      "\n",
      "Interval 9 (80000 steps performed)\n",
      "10000/10000 [==============================] - 19s 2ms/step - reward: 1.0000\n",
      "50 episodes - episode_reward: 200.000 [200.000, 200.000] - loss: 13.656 - mean_absolute_error: 37.854 - mean_q: 75.478\n",
      "\n",
      "Interval 10 (90000 steps performed)\n",
      "10000/10000 [==============================] - 19s 2ms/step - reward: 1.0000\n",
      "done, took 187.692 seconds\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1eb8a291630>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Training with verbose and visualization\n",
    "dqn.fit(env, nb_steps=100000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing for 20 episodes ...\n",
      "Episode 1: reward: 200.000, steps: 200\n",
      "Episode 2: reward: 200.000, steps: 200\n",
      "Episode 3: reward: 200.000, steps: 200\n",
      "Episode 4: reward: 200.000, steps: 200\n",
      "Episode 5: reward: 200.000, steps: 200\n",
      "Episode 6: reward: 200.000, steps: 200\n",
      "Episode 7: reward: 200.000, steps: 200\n",
      "Episode 8: reward: 200.000, steps: 200\n",
      "Episode 9: reward: 200.000, steps: 200\n",
      "Episode 10: reward: 200.000, steps: 200\n",
      "Episode 11: reward: 200.000, steps: 200\n",
      "Episode 12: reward: 200.000, steps: 200\n",
      "Episode 13: reward: 200.000, steps: 200\n",
      "Episode 14: reward: 200.000, steps: 200\n",
      "Episode 15: reward: 200.000, steps: 200\n",
      "Episode 16: reward: 200.000, steps: 200\n",
      "Episode 17: reward: 200.000, steps: 200\n",
      "Episode 18: reward: 200.000, steps: 200\n",
      "Episode 19: reward: 200.000, steps: 200\n",
      "Episode 20: reward: 200.000, steps: 200\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1ebff27af60>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test the model\n",
    "dqn.test(env, nb_episodes=20, visualize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion: \n",
    "\n",
    "* A DQN trained only 5000 times performs fairly well, managing to earn a max reward of 79.\n",
    "* A DQN trained 100,000 times, on the other hand, performed exceedingly well. In this scenario, the agent managed to reach the maximum number of steps per episode, 200, each episode."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
