{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Q Networks\n",
    "\n",
    "Code can be found at: https://www.analyticsvidhya.com/blog/2019/04/introduction-deep-q-learning-python/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import gym\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, Flatten\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "from rl.agents.dqn import DQNAgent\n",
    "from rl.policy import EpsGreedyQPolicy\n",
    "from rl.memory import SequentialMemory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the environment and extract the number of actions available\n",
    "env = gym.make('CartPole-v0')\n",
    "np.random.seed(0)\n",
    "env.seed(0)\n",
    "nb_actions = env.action_space.n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Testing with no AI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Richard\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\gym\\logger.py:30: UserWarning: \u001b[33mWARN: You are calling 'step()' even though this environment has already returned done = True. You should always call 'reset()' once you receive 'done = True' -- any further steps are undefined behavior.\u001b[0m\n",
      "  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    }
   ],
   "source": [
    "env.reset()\n",
    "for _ in range(150):\n",
    "    env.render()\n",
    "    env.step(env.action_space.sample()) # take a random action\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Create Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Richard\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "flatten_1 (Flatten)          (None, 4)                 0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 16)                80        \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 16)                0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 2)                 34        \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 2)                 0         \n",
      "=================================================================\n",
      "Total params: 114\n",
      "Trainable params: 114\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Flatten(input_shape=(1,) + env.observation_space.shape))\n",
    "model.add(Dense(16))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dense(nb_actions))\n",
    "model.add(Activation('linear'))\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Train and Test a simple DQN (5,000 iterations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initiate Parameters\n",
    "policy = EpsGreedyQPolicy()\n",
    "memory = SequentialMemory(limit=50000, window_length=1)\n",
    "dqn = DQNAgent(model=model, nb_actions=nb_actions, memory=memory, nb_steps_warmup=10,\n",
    "target_model_update=1e-2, policy=policy)\n",
    "dqn.compile(Adam(lr=1e-3), metrics=['mae'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train 5,000 Times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for 5000 steps ...\n",
      "   10/5000: episode: 1, duration: 0.260s, episode steps: 10, steps per second: 38, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.149 [-1.938, 3.116], loss: --, mean_absolute_error: --, mean_q: --\n",
      "WARNING:tensorflow:From C:\\Users\\Richard\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Richard\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\rl\\memory.py:39: UserWarning: Not enough entries to sample without replacement. Consider increasing your warm-up phase to avoid oversampling!\n",
      "  warnings.warn('Not enough entries to sample without replacement. Consider increasing your warm-up phase to avoid oversampling!')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   24/5000: episode: 2, duration: 0.654s, episode steps: 14, steps per second: 21, episode reward: 14.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.143 [0.000, 1.000], mean observation: 0.089 [-1.948, 3.035], loss: 0.589399, mean_absolute_error: 0.758852, mean_q: 0.680146\n",
      "   34/5000: episode: 3, duration: 0.165s, episode steps: 10, steps per second: 60, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.100 [0.000, 1.000], mean observation: 0.132 [-1.556, 2.591], loss: 0.527845, mean_absolute_error: 0.736724, mean_q: 0.767827\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Richard\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\rl\\memory.py:39: UserWarning: Not enough entries to sample without replacement. Consider increasing your warm-up phase to avoid oversampling!\n",
      "  warnings.warn('Not enough entries to sample without replacement. Consider increasing your warm-up phase to avoid oversampling!')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   46/5000: episode: 4, duration: 0.200s, episode steps: 12, steps per second: 60, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.083 [0.000, 1.000], mean observation: 0.122 [-1.911, 3.024], loss: 0.474312, mean_absolute_error: 0.708733, mean_q: 0.845246\n",
      "   56/5000: episode: 5, duration: 0.166s, episode steps: 10, steps per second: 60, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.100 [0.000, 1.000], mean observation: 0.136 [-1.615, 2.548], loss: 0.444827, mean_absolute_error: 0.703594, mean_q: 0.985417\n",
      "   65/5000: episode: 6, duration: 0.147s, episode steps: 9, steps per second: 61, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.160 [-1.733, 2.800], loss: 0.544864, mean_absolute_error: 0.740440, mean_q: 1.102551\n",
      "   76/5000: episode: 7, duration: 0.183s, episode steps: 11, steps per second: 60, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.182 [0.000, 1.000], mean observation: 0.122 [-1.589, 2.459], loss: 0.451784, mean_absolute_error: 0.676120, mean_q: 1.135695\n",
      "   86/5000: episode: 8, duration: 0.166s, episode steps: 10, steps per second: 60, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.159 [-1.915, 3.088], loss: 0.533011, mean_absolute_error: 0.720114, mean_q: 1.310124\n",
      "   95/5000: episode: 9, duration: 0.151s, episode steps: 9, steps per second: 60, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.137 [-1.776, 2.747], loss: 0.532453, mean_absolute_error: 0.706352, mean_q: 1.452468\n",
      "  106/5000: episode: 10, duration: 0.180s, episode steps: 11, steps per second: 61, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.141 [-2.101, 3.289], loss: 0.451705, mean_absolute_error: 0.613780, mean_q: 1.465818\n",
      "  116/5000: episode: 11, duration: 0.166s, episode steps: 10, steps per second: 60, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.100 [0.000, 1.000], mean observation: 0.138 [-1.593, 2.604], loss: 0.456003, mean_absolute_error: 0.575709, mean_q: 1.541701\n",
      "  127/5000: episode: 12, duration: 0.199s, episode steps: 11, steps per second: 55, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.144 [-2.106, 3.293], loss: 0.467088, mean_absolute_error: 0.563981, mean_q: 1.668658\n",
      "  139/5000: episode: 13, duration: 0.199s, episode steps: 12, steps per second: 60, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.083 [0.000, 1.000], mean observation: 0.128 [-1.974, 3.081], loss: 0.404409, mean_absolute_error: 0.502205, mean_q: 1.771753\n",
      "  147/5000: episode: 14, duration: 0.131s, episode steps: 8, steps per second: 61, episode reward: 8.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.160 [-1.525, 2.529], loss: 0.448995, mean_absolute_error: 0.487332, mean_q: 1.792817\n",
      "  157/5000: episode: 15, duration: 0.164s, episode steps: 10, steps per second: 61, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.132 [-1.990, 3.021], loss: 0.504216, mean_absolute_error: 0.512176, mean_q: 1.883247\n",
      "  167/5000: episode: 16, duration: 0.168s, episode steps: 10, steps per second: 60, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.100 [0.000, 1.000], mean observation: 0.116 [-1.611, 2.552], loss: 0.452630, mean_absolute_error: 0.486406, mean_q: 1.928043\n",
      "  177/5000: episode: 17, duration: 0.166s, episode steps: 10, steps per second: 60, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.153 [-1.927, 3.031], loss: 0.438205, mean_absolute_error: 0.483153, mean_q: 1.941635\n",
      "  189/5000: episode: 18, duration: 0.199s, episode steps: 12, steps per second: 60, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.167 [0.000, 1.000], mean observation: 0.097 [-1.581, 2.530], loss: 0.451121, mean_absolute_error: 0.515895, mean_q: 2.114444\n",
      "  198/5000: episode: 19, duration: 0.153s, episode steps: 9, steps per second: 59, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.170 [-1.713, 2.874], loss: 0.417851, mean_absolute_error: 0.576534, mean_q: 2.107095\n",
      "  209/5000: episode: 20, duration: 0.177s, episode steps: 11, steps per second: 62, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.091 [0.000, 1.000], mean observation: 0.123 [-1.731, 2.730], loss: 0.420858, mean_absolute_error: 0.597509, mean_q: 2.210026\n",
      "  220/5000: episode: 21, duration: 0.183s, episode steps: 11, steps per second: 60, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.091 [0.000, 1.000], mean observation: 0.137 [-1.751, 2.840], loss: 0.514139, mean_absolute_error: 0.712570, mean_q: 2.278774\n",
      "  231/5000: episode: 22, duration: 0.176s, episode steps: 11, steps per second: 63, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.091 [0.000, 1.000], mean observation: 0.106 [-1.761, 2.731], loss: 0.496085, mean_absolute_error: 0.755903, mean_q: 2.305939\n",
      "  240/5000: episode: 23, duration: 0.155s, episode steps: 9, steps per second: 58, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.150 [-1.773, 2.871], loss: 0.430774, mean_absolute_error: 0.793828, mean_q: 2.309990\n",
      "  250/5000: episode: 24, duration: 0.166s, episode steps: 10, steps per second: 60, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.154 [-1.925, 3.089], loss: 0.512243, mean_absolute_error: 0.875949, mean_q: 2.454069\n",
      "  260/5000: episode: 25, duration: 0.165s, episode steps: 10, steps per second: 61, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.100 [0.000, 1.000], mean observation: 0.120 [-1.589, 2.585], loss: 0.460549, mean_absolute_error: 0.890837, mean_q: 2.470123\n",
      "  269/5000: episode: 26, duration: 0.150s, episode steps: 9, steps per second: 60, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.111 [0.000, 1.000], mean observation: 0.131 [-1.580, 2.469], loss: 0.445731, mean_absolute_error: 0.916066, mean_q: 2.521051\n",
      "  279/5000: episode: 27, duration: 0.164s, episode steps: 10, steps per second: 61, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.135 [-1.905, 2.999], loss: 0.440981, mean_absolute_error: 0.978012, mean_q: 2.638702\n",
      "  289/5000: episode: 28, duration: 0.166s, episode steps: 10, steps per second: 60, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.153 [-1.934, 3.075], loss: 0.436453, mean_absolute_error: 1.017896, mean_q: 2.679465\n",
      "  298/5000: episode: 29, duration: 0.150s, episode steps: 9, steps per second: 60, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.111 [0.000, 1.000], mean observation: 0.137 [-1.374, 2.298], loss: 0.417544, mean_absolute_error: 1.055813, mean_q: 2.778887\n",
      "  307/5000: episode: 30, duration: 0.148s, episode steps: 9, steps per second: 61, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.142 [-1.775, 2.818], loss: 0.541239, mean_absolute_error: 1.137070, mean_q: 2.896306\n",
      "  317/5000: episode: 31, duration: 0.166s, episode steps: 10, steps per second: 60, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.100 [0.000, 1.000], mean observation: 0.159 [-1.710, 2.725], loss: 0.411991, mean_absolute_error: 1.146896, mean_q: 2.870829\n",
      "  327/5000: episode: 32, duration: 0.166s, episode steps: 10, steps per second: 60, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.100 [0.000, 1.000], mean observation: 0.097 [-1.615, 2.441], loss: 0.381330, mean_absolute_error: 1.176564, mean_q: 2.952627\n",
      "  337/5000: episode: 33, duration: 0.161s, episode steps: 10, steps per second: 62, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.121 [-2.002, 3.032], loss: 0.478336, mean_absolute_error: 1.245819, mean_q: 3.112478\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  349/5000: episode: 34, duration: 0.206s, episode steps: 12, steps per second: 58, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.083 [0.000, 1.000], mean observation: 0.094 [-1.986, 3.041], loss: 0.425101, mean_absolute_error: 1.285824, mean_q: 3.054163\n",
      "  358/5000: episode: 35, duration: 0.148s, episode steps: 9, steps per second: 61, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.146 [-1.745, 2.772], loss: 0.345916, mean_absolute_error: 1.302348, mean_q: 3.080801\n",
      "  367/5000: episode: 36, duration: 0.148s, episode steps: 9, steps per second: 61, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.121 [-1.805, 2.800], loss: 0.446412, mean_absolute_error: 1.367441, mean_q: 3.183436\n",
      "  376/5000: episode: 37, duration: 0.149s, episode steps: 9, steps per second: 61, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.155 [-1.724, 2.774], loss: 0.394321, mean_absolute_error: 1.381315, mean_q: 3.158121\n",
      "  386/5000: episode: 38, duration: 0.164s, episode steps: 10, steps per second: 61, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.127 [-1.984, 3.071], loss: 0.411759, mean_absolute_error: 1.389239, mean_q: 3.218243\n",
      "  395/5000: episode: 39, duration: 0.152s, episode steps: 9, steps per second: 59, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.147 [-1.755, 2.781], loss: 0.428504, mean_absolute_error: 1.437942, mean_q: 3.230962\n",
      "  404/5000: episode: 40, duration: 0.149s, episode steps: 9, steps per second: 61, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.111 [0.000, 1.000], mean observation: 0.141 [-1.517, 2.461], loss: 0.422503, mean_absolute_error: 1.459801, mean_q: 3.278723\n",
      "  413/5000: episode: 41, duration: 0.149s, episode steps: 9, steps per second: 60, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.135 [-1.741, 2.791], loss: 0.413120, mean_absolute_error: 1.485821, mean_q: 3.333552\n",
      "  423/5000: episode: 42, duration: 0.166s, episode steps: 10, steps per second: 60, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.134 [-1.979, 3.040], loss: 0.341363, mean_absolute_error: 1.473493, mean_q: 3.406274\n",
      "  432/5000: episode: 43, duration: 0.148s, episode steps: 9, steps per second: 61, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.173 [-1.741, 2.845], loss: 0.392558, mean_absolute_error: 1.511076, mean_q: 3.429767\n",
      "  442/5000: episode: 44, duration: 0.165s, episode steps: 10, steps per second: 61, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.145 [-1.913, 3.044], loss: 0.320631, mean_absolute_error: 1.482567, mean_q: 3.506179\n",
      "  452/5000: episode: 45, duration: 0.166s, episode steps: 10, steps per second: 60, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.142 [-1.929, 3.022], loss: 0.379288, mean_absolute_error: 1.556314, mean_q: 3.545763\n",
      "  462/5000: episode: 46, duration: 0.163s, episode steps: 10, steps per second: 61, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.130 [-1.968, 3.061], loss: 0.350892, mean_absolute_error: 1.553454, mean_q: 3.609957\n",
      "  470/5000: episode: 47, duration: 0.138s, episode steps: 8, steps per second: 58, episode reward: 8.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.147 [-1.549, 2.517], loss: 0.402569, mean_absolute_error: 1.598317, mean_q: 3.646722\n",
      "  489/5000: episode: 48, duration: 0.313s, episode steps: 19, steps per second: 61, episode reward: 19.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.211 [0.000, 1.000], mean observation: 0.059 [-2.170, 3.249], loss: 0.387126, mean_absolute_error: 1.627144, mean_q: 3.679915\n",
      "  499/5000: episode: 49, duration: 0.165s, episode steps: 10, steps per second: 61, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.100 [0.000, 1.000], mean observation: 0.136 [-1.620, 2.670], loss: 0.405107, mean_absolute_error: 1.689828, mean_q: 3.689549\n",
      "  508/5000: episode: 50, duration: 0.145s, episode steps: 9, steps per second: 62, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.161 [-1.734, 2.871], loss: 0.345414, mean_absolute_error: 1.683441, mean_q: 3.762151\n",
      "  516/5000: episode: 51, duration: 0.133s, episode steps: 8, steps per second: 60, episode reward: 8.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.125 [0.000, 1.000], mean observation: 0.131 [-1.166, 1.977], loss: 0.355706, mean_absolute_error: 1.711673, mean_q: 3.895554\n",
      "  526/5000: episode: 52, duration: 0.164s, episode steps: 10, steps per second: 61, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.100 [0.000, 1.000], mean observation: 0.123 [-1.567, 2.529], loss: 0.467872, mean_absolute_error: 1.837454, mean_q: 3.830190\n",
      "  534/5000: episode: 53, duration: 0.132s, episode steps: 8, steps per second: 61, episode reward: 8.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.151 [-1.532, 2.544], loss: 0.189348, mean_absolute_error: 1.738494, mean_q: 3.899561\n",
      "  542/5000: episode: 54, duration: 0.132s, episode steps: 8, steps per second: 61, episode reward: 8.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.163 [-1.540, 2.554], loss: 0.314130, mean_absolute_error: 1.825011, mean_q: 3.976706\n",
      "  551/5000: episode: 55, duration: 0.149s, episode steps: 9, steps per second: 60, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.111 [0.000, 1.000], mean observation: 0.139 [-1.579, 2.481], loss: 0.364720, mean_absolute_error: 1.874946, mean_q: 4.017019\n",
      "  561/5000: episode: 56, duration: 0.166s, episode steps: 10, steps per second: 60, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.200 [0.000, 1.000], mean observation: 0.142 [-1.542, 2.382], loss: 0.376401, mean_absolute_error: 1.919904, mean_q: 4.022489\n",
      "  572/5000: episode: 57, duration: 0.182s, episode steps: 11, steps per second: 60, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.182 [0.000, 1.000], mean observation: 0.086 [-1.605, 2.264], loss: 0.261457, mean_absolute_error: 1.900161, mean_q: 4.055479\n",
      "  582/5000: episode: 58, duration: 0.165s, episode steps: 10, steps per second: 60, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.200 [0.000, 1.000], mean observation: 0.133 [-1.412, 2.221], loss: 0.259725, mean_absolute_error: 1.932369, mean_q: 4.174060\n",
      "  594/5000: episode: 59, duration: 0.199s, episode steps: 12, steps per second: 60, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.250 [0.000, 1.000], mean observation: 0.118 [-1.372, 2.199], loss: 0.309820, mean_absolute_error: 1.972327, mean_q: 4.235249\n",
      "  604/5000: episode: 60, duration: 0.164s, episode steps: 10, steps per second: 61, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.200 [0.000, 1.000], mean observation: 0.127 [-1.592, 2.380], loss: 0.324352, mean_absolute_error: 1.985785, mean_q: 4.203852\n",
      "  613/5000: episode: 61, duration: 0.152s, episode steps: 9, steps per second: 59, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.222 [0.000, 1.000], mean observation: 0.136 [-1.355, 2.156], loss: 0.288052, mean_absolute_error: 1.986787, mean_q: 4.264425\n",
      "  625/5000: episode: 62, duration: 0.199s, episode steps: 12, steps per second: 60, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.167 [0.000, 1.000], mean observation: 0.101 [-1.758, 2.652], loss: 0.338833, mean_absolute_error: 2.024813, mean_q: 4.316975\n",
      "  635/5000: episode: 63, duration: 0.166s, episode steps: 10, steps per second: 60, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.200 [0.000, 1.000], mean observation: 0.140 [-1.539, 2.420], loss: 0.213829, mean_absolute_error: 1.995082, mean_q: 4.485233\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  645/5000: episode: 64, duration: 0.162s, episode steps: 10, steps per second: 62, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.100 [0.000, 1.000], mean observation: 0.119 [-1.742, 2.611], loss: 0.303750, mean_absolute_error: 2.057117, mean_q: 4.493590\n",
      "  654/5000: episode: 65, duration: 0.148s, episode steps: 9, steps per second: 61, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.111 [0.000, 1.000], mean observation: 0.140 [-1.609, 2.505], loss: 0.360788, mean_absolute_error: 2.108777, mean_q: 4.543900\n",
      "  663/5000: episode: 66, duration: 0.149s, episode steps: 9, steps per second: 60, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.111 [0.000, 1.000], mean observation: 0.151 [-1.348, 2.294], loss: 0.332103, mean_absolute_error: 2.104213, mean_q: 4.419550\n",
      "  671/5000: episode: 67, duration: 0.131s, episode steps: 8, steps per second: 61, episode reward: 8.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.125 [0.000, 1.000], mean observation: 0.141 [-1.386, 2.172], loss: 0.287076, mean_absolute_error: 2.124272, mean_q: 4.602043\n",
      "  679/5000: episode: 68, duration: 0.133s, episode steps: 8, steps per second: 60, episode reward: 8.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.125 [0.000, 1.000], mean observation: 0.135 [-1.396, 2.228], loss: 0.364380, mean_absolute_error: 2.168764, mean_q: 4.555245\n",
      "  687/5000: episode: 69, duration: 0.132s, episode steps: 8, steps per second: 61, episode reward: 8.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.144 [-1.536, 2.528], loss: 0.300293, mean_absolute_error: 2.154143, mean_q: 4.620807\n",
      "  699/5000: episode: 70, duration: 0.199s, episode steps: 12, steps per second: 60, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.250 [0.000, 1.000], mean observation: 0.103 [-1.556, 2.369], loss: 0.372563, mean_absolute_error: 2.184412, mean_q: 4.641838\n",
      "  709/5000: episode: 71, duration: 0.165s, episode steps: 10, steps per second: 61, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.100 [0.000, 1.000], mean observation: 0.140 [-1.785, 2.713], loss: 0.280531, mean_absolute_error: 2.177234, mean_q: 4.741097\n",
      "  720/5000: episode: 72, duration: 0.183s, episode steps: 11, steps per second: 60, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.182 [0.000, 1.000], mean observation: 0.121 [-1.609, 2.554], loss: 0.319178, mean_absolute_error: 2.191138, mean_q: 4.699008\n",
      "  729/5000: episode: 73, duration: 0.148s, episode steps: 9, steps per second: 61, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.111 [0.000, 1.000], mean observation: 0.130 [-1.599, 2.462], loss: 0.325008, mean_absolute_error: 2.216532, mean_q: 4.812663\n",
      "  739/5000: episode: 74, duration: 0.166s, episode steps: 10, steps per second: 60, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.100 [0.000, 1.000], mean observation: 0.160 [-1.537, 2.664], loss: 0.340308, mean_absolute_error: 2.221342, mean_q: 4.853230\n",
      "  752/5000: episode: 75, duration: 0.216s, episode steps: 13, steps per second: 60, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.154 [0.000, 1.000], mean observation: 0.114 [-1.752, 2.766], loss: 0.285842, mean_absolute_error: 2.208411, mean_q: 4.904661\n",
      "  763/5000: episode: 76, duration: 0.182s, episode steps: 11, steps per second: 61, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.091 [0.000, 1.000], mean observation: 0.129 [-1.712, 2.734], loss: 0.353123, mean_absolute_error: 2.228361, mean_q: 4.810405\n",
      "  773/5000: episode: 77, duration: 0.166s, episode steps: 10, steps per second: 60, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.100 [0.000, 1.000], mean observation: 0.146 [-1.707, 2.725], loss: 0.311379, mean_absolute_error: 2.220229, mean_q: 4.896925\n",
      "  782/5000: episode: 78, duration: 0.148s, episode steps: 9, steps per second: 61, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.157 [-1.717, 2.780], loss: 0.242880, mean_absolute_error: 2.231479, mean_q: 5.020598\n",
      "  794/5000: episode: 79, duration: 0.199s, episode steps: 12, steps per second: 60, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.167 [0.000, 1.000], mean observation: 0.087 [-1.770, 2.660], loss: 0.329871, mean_absolute_error: 2.276093, mean_q: 5.012499\n",
      "  806/5000: episode: 80, duration: 0.202s, episode steps: 12, steps per second: 59, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.250 [0.000, 1.000], mean observation: 0.099 [-1.414, 2.222], loss: 0.274064, mean_absolute_error: 2.269657, mean_q: 5.036091\n",
      "  815/5000: episode: 81, duration: 0.147s, episode steps: 9, steps per second: 61, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.146 [-1.744, 2.839], loss: 0.282906, mean_absolute_error: 2.308688, mean_q: 5.201696\n",
      "  825/5000: episode: 82, duration: 0.164s, episode steps: 10, steps per second: 61, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.100 [0.000, 1.000], mean observation: 0.147 [-1.773, 2.756], loss: 0.241369, mean_absolute_error: 2.303478, mean_q: 5.237135\n",
      "  836/5000: episode: 83, duration: 0.182s, episode steps: 11, steps per second: 60, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.091 [0.000, 1.000], mean observation: 0.132 [-1.916, 2.949], loss: 0.320877, mean_absolute_error: 2.358111, mean_q: 5.194602\n",
      "  850/5000: episode: 84, duration: 0.234s, episode steps: 14, steps per second: 60, episode reward: 14.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.143 [0.000, 1.000], mean observation: 0.109 [-1.903, 3.066], loss: 0.240806, mean_absolute_error: 2.357218, mean_q: 5.234782\n",
      "  860/5000: episode: 85, duration: 0.165s, episode steps: 10, steps per second: 61, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.174 [-1.915, 3.100], loss: 0.249359, mean_absolute_error: 2.399061, mean_q: 5.377926\n",
      "  870/5000: episode: 86, duration: 0.166s, episode steps: 10, steps per second: 60, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.129 [-1.988, 3.029], loss: 0.220378, mean_absolute_error: 2.385288, mean_q: 5.274586\n",
      "  881/5000: episode: 87, duration: 0.182s, episode steps: 11, steps per second: 60, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.091 [0.000, 1.000], mean observation: 0.134 [-1.961, 2.951], loss: 0.301272, mean_absolute_error: 2.454529, mean_q: 5.274672\n",
      "  891/5000: episode: 88, duration: 0.165s, episode steps: 10, steps per second: 61, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.100 [0.000, 1.000], mean observation: 0.137 [-1.565, 2.603], loss: 0.221754, mean_absolute_error: 2.380530, mean_q: 5.167266\n",
      "  902/5000: episode: 89, duration: 0.181s, episode steps: 11, steps per second: 61, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.182 [0.000, 1.000], mean observation: 0.133 [-1.568, 2.561], loss: 0.234835, mean_absolute_error: 2.461442, mean_q: 5.332956\n",
      "  912/5000: episode: 90, duration: 0.166s, episode steps: 10, steps per second: 60, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.100 [0.000, 1.000], mean observation: 0.157 [-1.745, 2.734], loss: 0.201288, mean_absolute_error: 2.507569, mean_q: 5.430967\n",
      "  924/5000: episode: 91, duration: 0.198s, episode steps: 12, steps per second: 61, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.250 [0.000, 1.000], mean observation: 0.128 [-1.368, 2.205], loss: 0.253266, mean_absolute_error: 2.568799, mean_q: 5.463264\n",
      "  933/5000: episode: 92, duration: 0.149s, episode steps: 9, steps per second: 60, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.111 [0.000, 1.000], mean observation: 0.155 [-1.550, 2.466], loss: 0.194628, mean_absolute_error: 2.585196, mean_q: 5.499772\n",
      "  943/5000: episode: 93, duration: 0.163s, episode steps: 10, steps per second: 61, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.100 [0.000, 1.000], mean observation: 0.115 [-1.570, 2.489], loss: 0.215546, mean_absolute_error: 2.549247, mean_q: 5.375841\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  952/5000: episode: 94, duration: 0.149s, episode steps: 9, steps per second: 60, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.146 [-1.737, 2.747], loss: 0.182172, mean_absolute_error: 2.595509, mean_q: 5.508574\n",
      "  960/5000: episode: 95, duration: 0.137s, episode steps: 8, steps per second: 58, episode reward: 8.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.125 [0.000, 1.000], mean observation: 0.134 [-1.213, 2.026], loss: 0.204866, mean_absolute_error: 2.575796, mean_q: 5.419012\n",
      "  971/5000: episode: 96, duration: 0.181s, episode steps: 11, steps per second: 61, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.182 [0.000, 1.000], mean observation: 0.145 [-1.512, 2.438], loss: 0.186981, mean_absolute_error: 2.637967, mean_q: 5.546784\n",
      "  981/5000: episode: 97, duration: 0.164s, episode steps: 10, steps per second: 61, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.200 [0.000, 1.000], mean observation: 0.138 [-1.369, 2.253], loss: 0.184930, mean_absolute_error: 2.609877, mean_q: 5.448938\n",
      "  989/5000: episode: 98, duration: 0.127s, episode steps: 8, steps per second: 63, episode reward: 8.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.125 [0.000, 1.000], mean observation: 0.145 [-1.406, 2.220], loss: 0.200673, mean_absolute_error: 2.661944, mean_q: 5.477325\n",
      "  999/5000: episode: 99, duration: 0.165s, episode steps: 10, steps per second: 60, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.200 [0.000, 1.000], mean observation: 0.123 [-1.399, 2.181], loss: 0.174129, mean_absolute_error: 2.663451, mean_q: 5.453329\n",
      " 1008/5000: episode: 100, duration: 0.149s, episode steps: 9, steps per second: 60, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.222 [0.000, 1.000], mean observation: 0.156 [-1.389, 2.208], loss: 0.173717, mean_absolute_error: 2.702159, mean_q: 5.600561\n",
      " 1018/5000: episode: 101, duration: 0.168s, episode steps: 10, steps per second: 60, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.200 [0.000, 1.000], mean observation: 0.107 [-1.609, 2.344], loss: 0.139478, mean_absolute_error: 2.776501, mean_q: 5.800163\n",
      " 1030/5000: episode: 102, duration: 0.196s, episode steps: 12, steps per second: 61, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.250 [0.000, 1.000], mean observation: 0.117 [-1.336, 2.237], loss: 0.166454, mean_absolute_error: 2.723901, mean_q: 5.585289\n",
      " 1039/5000: episode: 103, duration: 0.152s, episode steps: 9, steps per second: 59, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.222 [0.000, 1.000], mean observation: 0.133 [-1.412, 2.171], loss: 0.148918, mean_absolute_error: 2.765724, mean_q: 5.660225\n",
      " 1050/5000: episode: 104, duration: 0.179s, episode steps: 11, steps per second: 62, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.273 [0.000, 1.000], mean observation: 0.143 [-1.338, 2.205], loss: 0.179826, mean_absolute_error: 2.764483, mean_q: 5.609941\n",
      " 1059/5000: episode: 105, duration: 0.149s, episode steps: 9, steps per second: 60, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.222 [0.000, 1.000], mean observation: 0.146 [-1.403, 2.208], loss: 0.140904, mean_absolute_error: 2.821570, mean_q: 5.744557\n",
      " 1070/5000: episode: 106, duration: 0.188s, episode steps: 11, steps per second: 58, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.364 [0.000, 1.000], mean observation: 0.106 [-1.218, 1.877], loss: 0.151952, mean_absolute_error: 2.803472, mean_q: 5.674799\n",
      " 1080/5000: episode: 107, duration: 0.160s, episode steps: 10, steps per second: 62, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.200 [0.000, 1.000], mean observation: 0.146 [-1.550, 2.390], loss: 0.122294, mean_absolute_error: 2.818964, mean_q: 5.713775\n",
      " 1091/5000: episode: 108, duration: 0.182s, episode steps: 11, steps per second: 60, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.273 [0.000, 1.000], mean observation: 0.116 [-1.323, 2.088], loss: 0.167724, mean_absolute_error: 2.770994, mean_q: 5.504171\n",
      " 1100/5000: episode: 109, duration: 0.146s, episode steps: 9, steps per second: 62, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.333 [0.000, 1.000], mean observation: 0.132 [-1.176, 1.823], loss: 0.145921, mean_absolute_error: 2.880659, mean_q: 5.737997\n",
      " 1112/5000: episode: 110, duration: 0.199s, episode steps: 12, steps per second: 60, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.333 [0.000, 1.000], mean observation: 0.095 [-1.193, 1.808], loss: 0.159190, mean_absolute_error: 2.851958, mean_q: 5.627093\n",
      " 1124/5000: episode: 111, duration: 0.202s, episode steps: 12, steps per second: 60, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.333 [0.000, 1.000], mean observation: 0.110 [-1.332, 2.057], loss: 0.132410, mean_absolute_error: 2.989958, mean_q: 5.934011\n",
      " 1134/5000: episode: 112, duration: 0.165s, episode steps: 10, steps per second: 61, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.300 [0.000, 1.000], mean observation: 0.139 [-1.363, 2.117], loss: 0.193888, mean_absolute_error: 2.936105, mean_q: 5.738301\n",
      " 1143/5000: episode: 113, duration: 0.149s, episode steps: 9, steps per second: 61, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.333 [0.000, 1.000], mean observation: 0.123 [-1.191, 1.809], loss: 0.176455, mean_absolute_error: 2.944840, mean_q: 5.707253\n",
      " 1154/5000: episode: 114, duration: 0.183s, episode steps: 11, steps per second: 60, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.455 [0.000, 1.000], mean observation: 0.111 [-1.181, 1.762], loss: 0.139525, mean_absolute_error: 2.942239, mean_q: 5.754094\n",
      " 1164/5000: episode: 115, duration: 0.161s, episode steps: 10, steps per second: 62, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.400 [0.000, 1.000], mean observation: 0.140 [-0.940, 1.584], loss: 0.181270, mean_absolute_error: 2.946724, mean_q: 5.731979\n",
      " 1174/5000: episode: 116, duration: 0.165s, episode steps: 10, steps per second: 61, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.300 [0.000, 1.000], mean observation: 0.127 [-1.352, 2.108], loss: 0.171259, mean_absolute_error: 2.973699, mean_q: 5.743260\n",
      " 1186/5000: episode: 117, duration: 0.199s, episode steps: 12, steps per second: 60, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.417 [0.000, 1.000], mean observation: 0.107 [-1.330, 2.041], loss: 0.139193, mean_absolute_error: 3.002204, mean_q: 5.807613\n",
      " 1196/5000: episode: 118, duration: 0.170s, episode steps: 10, steps per second: 59, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.131 [-3.074, 1.984], loss: 0.154696, mean_absolute_error: 3.077696, mean_q: 5.929577\n",
      " 1206/5000: episode: 119, duration: 0.164s, episode steps: 10, steps per second: 61, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.127 [-2.975, 1.960], loss: 0.134547, mean_absolute_error: 3.061133, mean_q: 5.907414\n",
      " 1234/5000: episode: 120, duration: 0.464s, episode steps: 28, steps per second: 60, episode reward: 28.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.750 [0.000, 1.000], mean observation: 0.011 [-3.515, 2.669], loss: 0.179549, mean_absolute_error: 3.106205, mean_q: 5.909756\n",
      " 1243/5000: episode: 121, duration: 0.146s, episode steps: 9, steps per second: 62, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.150 [-2.827, 1.752], loss: 0.591001, mean_absolute_error: 3.188609, mean_q: 6.059621\n",
      " 1255/5000: episode: 122, duration: 0.199s, episode steps: 12, steps per second: 60, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.917 [0.000, 1.000], mean observation: -0.106 [-3.073, 1.949], loss: 0.210432, mean_absolute_error: 3.303379, mean_q: 6.348495\n",
      " 1266/5000: episode: 123, duration: 0.185s, episode steps: 11, steps per second: 59, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.122 [-3.280, 2.131], loss: 1.625953, mean_absolute_error: 3.403587, mean_q: 6.401387\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 1276/5000: episode: 124, duration: 0.166s, episode steps: 10, steps per second: 60, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.156 [-3.111, 1.985], loss: 0.615982, mean_absolute_error: 3.262220, mean_q: 6.140171\n",
      " 1288/5000: episode: 125, duration: 0.198s, episode steps: 12, steps per second: 60, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.917 [0.000, 1.000], mean observation: -0.115 [-3.030, 1.963], loss: 1.082268, mean_absolute_error: 3.377720, mean_q: 6.359297\n",
      " 1298/5000: episode: 126, duration: 0.169s, episode steps: 10, steps per second: 59, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.147 [-3.037, 1.924], loss: 0.632299, mean_absolute_error: 3.527035, mean_q: 6.675466\n",
      " 1322/5000: episode: 127, duration: 0.393s, episode steps: 24, steps per second: 61, episode reward: 24.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.750 [0.000, 1.000], mean observation: -0.045 [-3.411, 2.321], loss: 0.680974, mean_absolute_error: 3.452170, mean_q: 6.514531\n",
      " 1365/5000: episode: 128, duration: 0.712s, episode steps: 43, steps per second: 60, episode reward: 43.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.465 [0.000, 1.000], mean observation: 0.055 [-1.809, 2.339], loss: 1.479291, mean_absolute_error: 3.577153, mean_q: 6.685211\n",
      " 1418/5000: episode: 129, duration: 0.886s, episode steps: 53, steps per second: 60, episode reward: 53.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.585 [0.000, 1.000], mean observation: 0.270 [-0.962, 1.589], loss: 1.631359, mean_absolute_error: 3.687566, mean_q: 6.901104\n",
      " 1430/5000: episode: 130, duration: 0.196s, episode steps: 12, steps per second: 61, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.095 [-0.833, 1.251], loss: 1.794022, mean_absolute_error: 3.865307, mean_q: 7.169355\n",
      " 1444/5000: episode: 131, duration: 0.232s, episode steps: 14, steps per second: 60, episode reward: 14.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.095 [-0.984, 1.459], loss: 1.064195, mean_absolute_error: 3.656124, mean_q: 6.957642\n",
      " 1460/5000: episode: 132, duration: 0.265s, episode steps: 16, steps per second: 60, episode reward: 16.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.438 [0.000, 1.000], mean observation: 0.079 [-0.994, 1.419], loss: 1.884201, mean_absolute_error: 3.936769, mean_q: 7.356003\n",
      " 1477/5000: episode: 133, duration: 0.278s, episode steps: 17, steps per second: 61, episode reward: 17.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.471 [0.000, 1.000], mean observation: 0.084 [-0.963, 1.432], loss: 0.627587, mean_absolute_error: 3.768513, mean_q: 7.233105\n",
      " 1489/5000: episode: 134, duration: 0.202s, episode steps: 12, steps per second: 59, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.100 [-1.185, 1.801], loss: 0.549582, mean_absolute_error: 3.782208, mean_q: 7.160790\n",
      " 1553/5000: episode: 135, duration: 1.062s, episode steps: 64, steps per second: 60, episode reward: 64.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.578 [0.000, 1.000], mean observation: 0.295 [-0.805, 1.798], loss: 1.601217, mean_absolute_error: 3.974615, mean_q: 7.401968\n",
      " 1601/5000: episode: 136, duration: 0.799s, episode steps: 48, steps per second: 60, episode reward: 48.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.604 [0.000, 1.000], mean observation: 0.291 [-0.744, 1.866], loss: 1.209312, mean_absolute_error: 4.046421, mean_q: 7.597845\n",
      " 1655/5000: episode: 137, duration: 0.898s, episode steps: 54, steps per second: 60, episode reward: 54.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.574 [0.000, 1.000], mean observation: 0.236 [-0.766, 1.484], loss: 1.231906, mean_absolute_error: 4.163874, mean_q: 7.792341\n",
      " 1748/5000: episode: 138, duration: 1.549s, episode steps: 93, steps per second: 60, episode reward: 93.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.538 [0.000, 1.000], mean observation: 0.332 [-0.994, 1.626], loss: 1.300278, mean_absolute_error: 4.329759, mean_q: 8.083915\n",
      " 1947/5000: episode: 139, duration: 3.320s, episode steps: 199, steps per second: 60, episode reward: 199.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.528 [0.000, 1.000], mean observation: 0.362 [-0.669, 2.413], loss: 1.502594, mean_absolute_error: 4.731896, mean_q: 8.824202\n",
      " 2147/5000: episode: 140, duration: 3.347s, episode steps: 200, steps per second: 60, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.495 [0.000, 1.000], mean observation: -0.145 [-0.777, 0.566], loss: 1.693708, mean_absolute_error: 5.279502, mean_q: 9.922090\n",
      " 2261/5000: episode: 141, duration: 1.892s, episode steps: 114, steps per second: 60, episode reward: 114.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.482 [0.000, 1.000], mean observation: -0.351 [-1.743, 1.025], loss: 1.581715, mean_absolute_error: 5.604880, mean_q: 10.619606\n",
      " 2275/5000: episode: 142, duration: 0.233s, episode steps: 14, steps per second: 60, episode reward: 14.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.086 [-1.187, 0.829], loss: 2.066204, mean_absolute_error: 5.666890, mean_q: 10.791183\n",
      " 2293/5000: episode: 143, duration: 0.300s, episode steps: 18, steps per second: 60, episode reward: 18.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.084 [-1.410, 0.928], loss: 1.796109, mean_absolute_error: 5.739314, mean_q: 10.928583\n",
      " 2312/5000: episode: 144, duration: 0.315s, episode steps: 19, steps per second: 60, episode reward: 19.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.526 [0.000, 1.000], mean observation: -0.066 [-1.152, 0.814], loss: 1.162353, mean_absolute_error: 5.745652, mean_q: 11.046287\n",
      " 2329/5000: episode: 145, duration: 0.280s, episode steps: 17, steps per second: 61, episode reward: 17.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.588 [0.000, 1.000], mean observation: -0.032 [-1.608, 1.213], loss: 1.685504, mean_absolute_error: 5.860291, mean_q: 11.231654\n",
      " 2341/5000: episode: 146, duration: 0.200s, episode steps: 12, steps per second: 60, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.583 [0.000, 1.000], mean observation: -0.132 [-1.529, 0.933], loss: 1.220311, mean_absolute_error: 5.853799, mean_q: 11.216643\n",
      " 2355/5000: episode: 147, duration: 0.231s, episode steps: 14, steps per second: 61, episode reward: 14.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.571 [0.000, 1.000], mean observation: -0.091 [-1.680, 1.145], loss: 1.147881, mean_absolute_error: 5.808667, mean_q: 11.214293\n",
      " 2366/5000: episode: 148, duration: 0.182s, episode steps: 11, steps per second: 61, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.545 [0.000, 1.000], mean observation: -0.115 [-1.259, 0.759], loss: 1.526644, mean_absolute_error: 5.953907, mean_q: 11.477917\n",
      " 2380/5000: episode: 149, duration: 0.229s, episode steps: 14, steps per second: 61, episode reward: 14.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.714 [0.000, 1.000], mean observation: -0.101 [-2.162, 1.370], loss: 1.943716, mean_absolute_error: 6.163182, mean_q: 11.828761\n",
      " 2391/5000: episode: 150, duration: 0.185s, episode steps: 11, steps per second: 59, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.727 [0.000, 1.000], mean observation: -0.086 [-1.987, 1.406], loss: 2.315215, mean_absolute_error: 6.326634, mean_q: 12.177094\n",
      " 2402/5000: episode: 151, duration: 0.180s, episode steps: 11, steps per second: 61, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.727 [0.000, 1.000], mean observation: -0.116 [-2.116, 1.343], loss: 2.794974, mean_absolute_error: 6.201767, mean_q: 11.873748\n",
      " 2412/5000: episode: 152, duration: 0.170s, episode steps: 10, steps per second: 59, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.800 [0.000, 1.000], mean observation: -0.137 [-2.419, 1.520], loss: 3.257675, mean_absolute_error: 6.199500, mean_q: 11.826299\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 2424/5000: episode: 153, duration: 0.198s, episode steps: 12, steps per second: 61, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.750 [0.000, 1.000], mean observation: -0.136 [-2.450, 1.518], loss: 2.141752, mean_absolute_error: 6.354073, mean_q: 12.150212\n",
      " 2434/5000: episode: 154, duration: 0.163s, episode steps: 10, steps per second: 61, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.900 [0.000, 1.000], mean observation: -0.140 [-2.532, 1.538], loss: 3.473845, mean_absolute_error: 6.361268, mean_q: 11.948492\n",
      " 2445/5000: episode: 155, duration: 0.181s, episode steps: 11, steps per second: 61, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.727 [0.000, 1.000], mean observation: -0.104 [-2.175, 1.414], loss: 2.899632, mean_absolute_error: 6.164294, mean_q: 11.625362\n",
      " 2454/5000: episode: 156, duration: 0.148s, episode steps: 9, steps per second: 61, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.169 [-2.854, 1.757], loss: 1.554650, mean_absolute_error: 6.332515, mean_q: 12.141795\n",
      " 2463/5000: episode: 157, duration: 0.149s, episode steps: 9, steps per second: 60, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.889 [0.000, 1.000], mean observation: -0.125 [-2.440, 1.564], loss: 4.121027, mean_absolute_error: 6.583703, mean_q: 12.389845\n",
      " 2472/5000: episode: 158, duration: 0.150s, episode steps: 9, steps per second: 60, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.133 [-2.820, 1.806], loss: 4.620001, mean_absolute_error: 6.637398, mean_q: 12.521627\n",
      " 2484/5000: episode: 159, duration: 0.195s, episode steps: 12, steps per second: 61, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.833 [0.000, 1.000], mean observation: -0.096 [-2.394, 1.567], loss: 1.210561, mean_absolute_error: 6.513997, mean_q: 12.600358\n",
      " 2493/5000: episode: 160, duration: 0.147s, episode steps: 9, steps per second: 61, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.889 [0.000, 1.000], mean observation: -0.132 [-2.257, 1.418], loss: 2.644377, mean_absolute_error: 6.512737, mean_q: 12.573491\n",
      " 2502/5000: episode: 161, duration: 0.148s, episode steps: 9, steps per second: 61, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.889 [0.000, 1.000], mean observation: -0.152 [-2.254, 1.331], loss: 2.755929, mean_absolute_error: 6.596091, mean_q: 12.691592\n",
      " 2512/5000: episode: 162, duration: 0.165s, episode steps: 10, steps per second: 61, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.800 [0.000, 1.000], mean observation: -0.115 [-2.040, 1.379], loss: 5.594832, mean_absolute_error: 7.018858, mean_q: 13.190474\n",
      " 2523/5000: episode: 163, duration: 0.182s, episode steps: 11, steps per second: 60, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.636 [0.000, 1.000], mean observation: -0.128 [-1.751, 1.134], loss: 2.069695, mean_absolute_error: 6.603971, mean_q: 12.547488\n",
      " 2535/5000: episode: 164, duration: 0.198s, episode steps: 12, steps per second: 61, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.667 [0.000, 1.000], mean observation: -0.099 [-1.466, 0.989], loss: 2.894521, mean_absolute_error: 6.669115, mean_q: 12.563016\n",
      " 2548/5000: episode: 165, duration: 0.213s, episode steps: 13, steps per second: 61, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.615 [0.000, 1.000], mean observation: -0.094 [-1.425, 0.991], loss: 3.843040, mean_absolute_error: 6.768977, mean_q: 12.648187\n",
      " 2560/5000: episode: 166, duration: 0.195s, episode steps: 12, steps per second: 61, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.583 [0.000, 1.000], mean observation: -0.128 [-1.330, 0.773], loss: 1.972794, mean_absolute_error: 6.719757, mean_q: 12.795781\n",
      " 2572/5000: episode: 167, duration: 0.199s, episode steps: 12, steps per second: 60, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.583 [0.000, 1.000], mean observation: -0.103 [-1.535, 0.999], loss: 4.166232, mean_absolute_error: 6.859783, mean_q: 12.769154\n",
      " 2585/5000: episode: 168, duration: 0.218s, episode steps: 13, steps per second: 60, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.538 [0.000, 1.000], mean observation: -0.091 [-1.227, 0.830], loss: 2.200473, mean_absolute_error: 6.688200, mean_q: 12.657438\n",
      " 2600/5000: episode: 169, duration: 0.248s, episode steps: 15, steps per second: 60, episode reward: 15.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.533 [0.000, 1.000], mean observation: -0.101 [-1.317, 0.765], loss: 5.649775, mean_absolute_error: 7.100576, mean_q: 13.124125\n",
      " 2612/5000: episode: 170, duration: 0.199s, episode steps: 12, steps per second: 60, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.583 [0.000, 1.000], mean observation: -0.126 [-1.464, 0.927], loss: 3.682139, mean_absolute_error: 6.830201, mean_q: 12.668115\n",
      " 2626/5000: episode: 171, duration: 0.229s, episode steps: 14, steps per second: 61, episode reward: 14.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.571 [0.000, 1.000], mean observation: -0.060 [-1.395, 1.000], loss: 3.361599, mean_absolute_error: 6.960769, mean_q: 12.983953\n",
      " 2638/5000: episode: 172, duration: 0.199s, episode steps: 12, steps per second: 60, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.583 [0.000, 1.000], mean observation: -0.086 [-1.459, 1.009], loss: 4.079668, mean_absolute_error: 7.072431, mean_q: 13.178451\n",
      " 2653/5000: episode: 173, duration: 0.252s, episode steps: 15, steps per second: 60, episode reward: 15.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.600 [0.000, 1.000], mean observation: -0.076 [-1.381, 0.939], loss: 2.421823, mean_absolute_error: 7.003585, mean_q: 13.325481\n",
      " 2665/5000: episode: 174, duration: 0.199s, episode steps: 12, steps per second: 60, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.583 [0.000, 1.000], mean observation: -0.104 [-1.476, 0.991], loss: 3.366583, mean_absolute_error: 7.273757, mean_q: 13.780850\n",
      " 2679/5000: episode: 175, duration: 0.228s, episode steps: 14, steps per second: 61, episode reward: 14.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.571 [0.000, 1.000], mean observation: -0.105 [-1.444, 0.940], loss: 3.550457, mean_absolute_error: 7.084072, mean_q: 13.335375\n",
      " 2692/5000: episode: 176, duration: 0.215s, episode steps: 13, steps per second: 60, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.615 [0.000, 1.000], mean observation: -0.082 [-1.433, 1.025], loss: 3.461527, mean_absolute_error: 7.120673, mean_q: 13.345565\n",
      " 2702/5000: episode: 177, duration: 0.165s, episode steps: 10, steps per second: 61, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.700 [0.000, 1.000], mean observation: -0.131 [-1.681, 0.963], loss: 3.827786, mean_absolute_error: 7.125257, mean_q: 13.387655\n",
      " 2714/5000: episode: 178, duration: 0.202s, episode steps: 12, steps per second: 59, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.667 [0.000, 1.000], mean observation: -0.094 [-1.814, 1.212], loss: 1.706035, mean_absolute_error: 7.011652, mean_q: 13.494231\n",
      " 2724/5000: episode: 179, duration: 0.165s, episode steps: 10, steps per second: 61, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.700 [0.000, 1.000], mean observation: -0.098 [-1.888, 1.204], loss: 3.800719, mean_absolute_error: 7.204749, mean_q: 13.598027\n",
      " 2734/5000: episode: 180, duration: 0.164s, episode steps: 10, steps per second: 61, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.800 [0.000, 1.000], mean observation: -0.115 [-2.414, 1.597], loss: 3.341686, mean_absolute_error: 7.127378, mean_q: 13.506805\n",
      " 2746/5000: episode: 181, duration: 0.197s, episode steps: 12, steps per second: 61, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.750 [0.000, 1.000], mean observation: -0.099 [-2.048, 1.359], loss: 3.154750, mean_absolute_error: 7.129354, mean_q: 13.499588\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 2756/5000: episode: 182, duration: 0.165s, episode steps: 10, steps per second: 61, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.800 [0.000, 1.000], mean observation: -0.143 [-2.189, 1.339], loss: 3.851773, mean_absolute_error: 7.215198, mean_q: 13.681656\n",
      " 2766/5000: episode: 183, duration: 0.168s, episode steps: 10, steps per second: 60, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.800 [0.000, 1.000], mean observation: -0.158 [-2.237, 1.356], loss: 5.431403, mean_absolute_error: 7.404189, mean_q: 13.753653\n",
      " 2774/5000: episode: 184, duration: 0.131s, episode steps: 8, steps per second: 61, episode reward: 8.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.875 [0.000, 1.000], mean observation: -0.159 [-2.214, 1.326], loss: 3.912998, mean_absolute_error: 7.150518, mean_q: 13.398586\n",
      " 2783/5000: episode: 185, duration: 0.149s, episode steps: 9, steps per second: 61, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.778 [0.000, 1.000], mean observation: -0.144 [-2.213, 1.367], loss: 3.811085, mean_absolute_error: 7.305082, mean_q: 13.706158\n",
      " 2792/5000: episode: 186, duration: 0.149s, episode steps: 9, steps per second: 60, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.889 [0.000, 1.000], mean observation: -0.157 [-2.430, 1.518], loss: 2.590108, mean_absolute_error: 7.244901, mean_q: 13.752766\n",
      " 2801/5000: episode: 187, duration: 0.146s, episode steps: 9, steps per second: 62, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.889 [0.000, 1.000], mean observation: -0.122 [-2.258, 1.394], loss: 4.185632, mean_absolute_error: 7.504911, mean_q: 14.123857\n",
      " 2814/5000: episode: 188, duration: 0.217s, episode steps: 13, steps per second: 60, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.769 [0.000, 1.000], mean observation: -0.098 [-2.348, 1.604], loss: 3.098247, mean_absolute_error: 7.353686, mean_q: 13.952179\n",
      " 2825/5000: episode: 189, duration: 0.182s, episode steps: 11, steps per second: 60, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.818 [0.000, 1.000], mean observation: -0.119 [-2.296, 1.531], loss: 1.859179, mean_absolute_error: 7.532193, mean_q: 14.569098\n",
      " 2835/5000: episode: 190, duration: 0.166s, episode steps: 10, steps per second: 60, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.800 [0.000, 1.000], mean observation: -0.147 [-2.390, 1.525], loss: 4.808772, mean_absolute_error: 7.446551, mean_q: 14.032367\n",
      " 2844/5000: episode: 191, duration: 0.149s, episode steps: 9, steps per second: 61, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.889 [0.000, 1.000], mean observation: -0.136 [-2.460, 1.607], loss: 4.408716, mean_absolute_error: 7.390399, mean_q: 13.892544\n",
      " 2856/5000: episode: 192, duration: 0.201s, episode steps: 12, steps per second: 60, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.833 [0.000, 1.000], mean observation: -0.111 [-2.498, 1.569], loss: 4.266653, mean_absolute_error: 7.657635, mean_q: 14.476228\n",
      " 2866/5000: episode: 193, duration: 0.163s, episode steps: 10, steps per second: 61, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.900 [0.000, 1.000], mean observation: -0.161 [-2.631, 1.525], loss: 4.949260, mean_absolute_error: 7.493062, mean_q: 14.045853\n",
      " 2875/5000: episode: 194, duration: 0.149s, episode steps: 9, steps per second: 61, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.150 [-2.785, 1.755], loss: 5.802328, mean_absolute_error: 7.743746, mean_q: 14.367284\n",
      " 2886/5000: episode: 195, duration: 0.181s, episode steps: 11, steps per second: 61, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.909 [0.000, 1.000], mean observation: -0.111 [-2.855, 1.799], loss: 2.912140, mean_absolute_error: 7.585763, mean_q: 14.331677\n",
      " 2896/5000: episode: 196, duration: 0.167s, episode steps: 10, steps per second: 60, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.900 [0.000, 1.000], mean observation: -0.121 [-2.594, 1.590], loss: 2.095791, mean_absolute_error: 7.590741, mean_q: 14.555430\n",
      " 2906/5000: episode: 197, duration: 0.180s, episode steps: 10, steps per second: 55, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.800 [0.000, 1.000], mean observation: -0.140 [-2.394, 1.545], loss: 4.140085, mean_absolute_error: 7.581632, mean_q: 14.295158\n",
      " 2915/5000: episode: 198, duration: 0.150s, episode steps: 9, steps per second: 60, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.778 [0.000, 1.000], mean observation: -0.134 [-2.165, 1.353], loss: 3.897830, mean_absolute_error: 7.571460, mean_q: 14.223151\n",
      " 2926/5000: episode: 199, duration: 0.181s, episode steps: 11, steps per second: 61, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.636 [0.000, 1.000], mean observation: -0.097 [-1.825, 1.220], loss: 4.282965, mean_absolute_error: 7.555127, mean_q: 14.127383\n",
      " 2936/5000: episode: 200, duration: 0.165s, episode steps: 10, steps per second: 61, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.700 [0.000, 1.000], mean observation: -0.104 [-1.854, 1.214], loss: 4.500523, mean_absolute_error: 7.757989, mean_q: 14.571783\n",
      " 2947/5000: episode: 201, duration: 0.182s, episode steps: 11, steps per second: 60, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.818 [0.000, 1.000], mean observation: -0.131 [-2.268, 1.334], loss: 3.733717, mean_absolute_error: 7.673279, mean_q: 14.470743\n",
      " 2957/5000: episode: 202, duration: 0.166s, episode steps: 10, steps per second: 60, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.900 [0.000, 1.000], mean observation: -0.149 [-2.465, 1.529], loss: 4.158164, mean_absolute_error: 7.758063, mean_q: 14.557248\n",
      " 2966/5000: episode: 203, duration: 0.148s, episode steps: 9, steps per second: 61, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.889 [0.000, 1.000], mean observation: -0.131 [-2.472, 1.568], loss: 4.993542, mean_absolute_error: 7.835424, mean_q: 14.597341\n",
      " 2976/5000: episode: 204, duration: 0.168s, episode steps: 10, steps per second: 60, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.900 [0.000, 1.000], mean observation: -0.097 [-2.634, 1.804], loss: 5.390767, mean_absolute_error: 7.823255, mean_q: 14.567663\n",
      " 2987/5000: episode: 205, duration: 0.180s, episode steps: 11, steps per second: 61, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.818 [0.000, 1.000], mean observation: -0.115 [-2.442, 1.607], loss: 3.908435, mean_absolute_error: 7.606689, mean_q: 14.339130\n",
      " 2999/5000: episode: 206, duration: 0.197s, episode steps: 12, steps per second: 61, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.833 [0.000, 1.000], mean observation: -0.122 [-2.721, 1.770], loss: 3.471008, mean_absolute_error: 7.627228, mean_q: 14.414359\n",
      " 3008/5000: episode: 207, duration: 0.149s, episode steps: 9, steps per second: 60, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.889 [0.000, 1.000], mean observation: -0.140 [-2.293, 1.380], loss: 3.944674, mean_absolute_error: 7.644239, mean_q: 14.379105\n",
      " 3019/5000: episode: 208, duration: 0.182s, episode steps: 11, steps per second: 61, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.818 [0.000, 1.000], mean observation: -0.114 [-2.334, 1.605], loss: 5.344857, mean_absolute_error: 8.114371, mean_q: 15.179886\n",
      " 3027/5000: episode: 209, duration: 0.148s, episode steps: 8, steps per second: 54, episode reward: 8.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.159 [-2.535, 1.550], loss: 5.194037, mean_absolute_error: 7.798122, mean_q: 14.464843\n",
      " 3037/5000: episode: 210, duration: 0.165s, episode steps: 10, steps per second: 60, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.800 [0.000, 1.000], mean observation: -0.122 [-2.177, 1.392], loss: 4.614180, mean_absolute_error: 7.760842, mean_q: 14.418943\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 3047/5000: episode: 211, duration: 0.166s, episode steps: 10, steps per second: 60, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.800 [0.000, 1.000], mean observation: -0.127 [-2.383, 1.549], loss: 3.057979, mean_absolute_error: 7.657626, mean_q: 14.509886\n",
      " 3056/5000: episode: 212, duration: 0.148s, episode steps: 9, steps per second: 61, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.889 [0.000, 1.000], mean observation: -0.134 [-2.413, 1.571], loss: 5.918040, mean_absolute_error: 7.990569, mean_q: 14.761852\n",
      " 3066/5000: episode: 213, duration: 0.165s, episode steps: 10, steps per second: 61, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.800 [0.000, 1.000], mean observation: -0.134 [-2.036, 1.172], loss: 2.467758, mean_absolute_error: 7.718955, mean_q: 14.715654\n",
      " 3076/5000: episode: 214, duration: 0.165s, episode steps: 10, steps per second: 61, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.900 [0.000, 1.000], mean observation: -0.153 [-2.776, 1.760], loss: 2.593976, mean_absolute_error: 7.628894, mean_q: 14.592166\n",
      " 3088/5000: episode: 215, duration: 0.196s, episode steps: 12, steps per second: 61, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.833 [0.000, 1.000], mean observation: -0.106 [-2.686, 1.761], loss: 4.790650, mean_absolute_error: 7.843586, mean_q: 14.840022\n",
      " 3097/5000: episode: 216, duration: 0.148s, episode steps: 9, steps per second: 61, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.889 [0.000, 1.000], mean observation: -0.143 [-2.495, 1.585], loss: 5.044416, mean_absolute_error: 7.974871, mean_q: 14.817280\n",
      " 3107/5000: episode: 217, duration: 0.167s, episode steps: 10, steps per second: 60, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.157 [-3.057, 1.915], loss: 5.492629, mean_absolute_error: 8.099802, mean_q: 14.983500\n",
      " 3117/5000: episode: 218, duration: 0.167s, episode steps: 10, steps per second: 60, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.900 [0.000, 1.000], mean observation: -0.142 [-2.536, 1.545], loss: 2.664249, mean_absolute_error: 7.641391, mean_q: 14.451897\n",
      " 3125/5000: episode: 219, duration: 0.132s, episode steps: 8, steps per second: 61, episode reward: 8.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.139 [-2.562, 1.590], loss: 6.262465, mean_absolute_error: 8.032194, mean_q: 14.722681\n",
      " 3134/5000: episode: 220, duration: 0.149s, episode steps: 9, steps per second: 61, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.889 [0.000, 1.000], mean observation: -0.141 [-2.489, 1.610], loss: 3.398045, mean_absolute_error: 7.683755, mean_q: 14.465188\n",
      " 3144/5000: episode: 221, duration: 0.165s, episode steps: 10, steps per second: 61, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.900 [0.000, 1.000], mean observation: -0.128 [-2.494, 1.553], loss: 5.166384, mean_absolute_error: 7.831670, mean_q: 14.548098\n",
      " 3153/5000: episode: 222, duration: 0.149s, episode steps: 9, steps per second: 60, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.166 [-2.864, 1.770], loss: 2.870484, mean_absolute_error: 7.705112, mean_q: 14.528619\n",
      " 3162/5000: episode: 223, duration: 0.145s, episode steps: 9, steps per second: 62, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.144 [-2.832, 1.749], loss: 3.758548, mean_absolute_error: 7.814733, mean_q: 14.697691\n",
      " 3171/5000: episode: 224, duration: 0.149s, episode steps: 9, steps per second: 60, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.163 [-2.794, 1.719], loss: 4.577844, mean_absolute_error: 7.857159, mean_q: 14.645342\n",
      " 3180/5000: episode: 225, duration: 0.149s, episode steps: 9, steps per second: 60, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.149 [-2.830, 1.738], loss: 3.971794, mean_absolute_error: 7.837780, mean_q: 14.523758\n",
      " 3189/5000: episode: 226, duration: 0.152s, episode steps: 9, steps per second: 59, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.889 [0.000, 1.000], mean observation: -0.130 [-2.471, 1.598], loss: 3.091174, mean_absolute_error: 7.737993, mean_q: 14.577940\n",
      " 3198/5000: episode: 227, duration: 0.148s, episode steps: 9, steps per second: 61, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.889 [0.000, 1.000], mean observation: -0.157 [-2.504, 1.556], loss: 3.813530, mean_absolute_error: 7.741573, mean_q: 14.491115\n",
      " 3208/5000: episode: 228, duration: 0.165s, episode steps: 10, steps per second: 61, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.900 [0.000, 1.000], mean observation: -0.115 [-2.591, 1.583], loss: 4.695759, mean_absolute_error: 7.840299, mean_q: 14.582750\n",
      " 3217/5000: episode: 229, duration: 0.145s, episode steps: 9, steps per second: 62, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.154 [-2.796, 1.730], loss: 6.338059, mean_absolute_error: 7.907418, mean_q: 14.466251\n",
      " 3227/5000: episode: 230, duration: 0.172s, episode steps: 10, steps per second: 58, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.900 [0.000, 1.000], mean observation: -0.130 [-2.470, 1.573], loss: 3.006490, mean_absolute_error: 7.845602, mean_q: 14.788518\n",
      " 3237/5000: episode: 231, duration: 0.162s, episode steps: 10, steps per second: 62, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.900 [0.000, 1.000], mean observation: -0.123 [-2.555, 1.578], loss: 3.563409, mean_absolute_error: 7.746480, mean_q: 14.470421\n",
      " 3248/5000: episode: 232, duration: 0.182s, episode steps: 11, steps per second: 60, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.818 [0.000, 1.000], mean observation: -0.133 [-2.282, 1.325], loss: 4.284665, mean_absolute_error: 7.812993, mean_q: 14.577289\n",
      " 3257/5000: episode: 233, duration: 0.148s, episode steps: 9, steps per second: 61, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.889 [0.000, 1.000], mean observation: -0.135 [-2.486, 1.611], loss: 3.727781, mean_absolute_error: 7.758887, mean_q: 14.515749\n",
      " 3267/5000: episode: 234, duration: 0.165s, episode steps: 10, steps per second: 61, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.145 [-3.086, 1.927], loss: 3.654242, mean_absolute_error: 7.744813, mean_q: 14.577970\n",
      " 3275/5000: episode: 235, duration: 0.124s, episode steps: 8, steps per second: 64, episode reward: 8.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.136 [-2.564, 1.585], loss: 3.259290, mean_absolute_error: 7.740000, mean_q: 14.462114\n",
      " 3285/5000: episode: 236, duration: 0.176s, episode steps: 10, steps per second: 57, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.108 [-2.967, 1.988], loss: 3.047458, mean_absolute_error: 7.618077, mean_q: 14.429454\n",
      " 3294/5000: episode: 237, duration: 0.144s, episode steps: 9, steps per second: 63, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.113 [-2.756, 1.805], loss: 4.293639, mean_absolute_error: 7.923039, mean_q: 14.877372\n",
      " 3303/5000: episode: 238, duration: 0.154s, episode steps: 9, steps per second: 58, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.166 [-2.866, 1.782], loss: 2.679027, mean_absolute_error: 7.706601, mean_q: 14.650349\n",
      " 3311/5000: episode: 239, duration: 0.128s, episode steps: 8, steps per second: 62, episode reward: 8.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.139 [-2.507, 1.550], loss: 2.840540, mean_absolute_error: 7.575047, mean_q: 14.377300\n",
      " 3320/5000: episode: 240, duration: 0.144s, episode steps: 9, steps per second: 62, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.778 [0.000, 1.000], mean observation: -0.102 [-2.100, 1.412], loss: 4.441866, mean_absolute_error: 7.935305, mean_q: 14.809526\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 3329/5000: episode: 241, duration: 0.149s, episode steps: 9, steps per second: 60, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.667 [0.000, 1.000], mean observation: -0.128 [-1.845, 1.158], loss: 4.914502, mean_absolute_error: 7.939067, mean_q: 14.761989\n",
      " 3339/5000: episode: 242, duration: 0.168s, episode steps: 10, steps per second: 60, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.700 [0.000, 1.000], mean observation: -0.139 [-1.833, 1.173], loss: 3.954776, mean_absolute_error: 7.430270, mean_q: 13.726990\n",
      " 3351/5000: episode: 243, duration: 0.196s, episode steps: 12, steps per second: 61, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.583 [0.000, 1.000], mean observation: -0.106 [-1.741, 1.165], loss: 2.447071, mean_absolute_error: 7.762321, mean_q: 14.575443\n",
      " 3364/5000: episode: 244, duration: 0.218s, episode steps: 13, steps per second: 60, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.615 [0.000, 1.000], mean observation: -0.075 [-1.698, 1.190], loss: 3.196159, mean_absolute_error: 7.684370, mean_q: 14.400965\n",
      " 3373/5000: episode: 245, duration: 0.148s, episode steps: 9, steps per second: 61, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.778 [0.000, 1.000], mean observation: -0.135 [-1.875, 1.218], loss: 2.299710, mean_absolute_error: 7.691778, mean_q: 14.557427\n",
      " 3382/5000: episode: 246, duration: 0.150s, episode steps: 9, steps per second: 60, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.778 [0.000, 1.000], mean observation: -0.140 [-1.898, 1.138], loss: 2.211972, mean_absolute_error: 7.648411, mean_q: 14.533768\n",
      " 3392/5000: episode: 247, duration: 0.165s, episode steps: 10, steps per second: 61, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.700 [0.000, 1.000], mean observation: -0.110 [-1.784, 1.149], loss: 3.482546, mean_absolute_error: 7.751173, mean_q: 14.414342\n",
      " 3402/5000: episode: 248, duration: 0.162s, episode steps: 10, steps per second: 62, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.700 [0.000, 1.000], mean observation: -0.122 [-1.856, 1.217], loss: 2.615951, mean_absolute_error: 7.813228, mean_q: 14.724643\n",
      " 3414/5000: episode: 249, duration: 0.205s, episode steps: 12, steps per second: 58, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.583 [0.000, 1.000], mean observation: -0.112 [-1.534, 0.940], loss: 3.969485, mean_absolute_error: 7.868158, mean_q: 14.587420\n",
      " 3435/5000: episode: 250, duration: 0.344s, episode steps: 21, steps per second: 61, episode reward: 21.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.476 [0.000, 1.000], mean observation: -0.048 [-1.374, 1.017], loss: 2.977967, mean_absolute_error: 7.632474, mean_q: 14.289778\n",
      " 3481/5000: episode: 251, duration: 0.765s, episode steps: 46, steps per second: 60, episode reward: 46.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.478 [0.000, 1.000], mean observation: -0.098 [-1.113, 0.805], loss: 3.364419, mean_absolute_error: 7.746573, mean_q: 14.430688\n",
      " 3564/5000: episode: 252, duration: 1.378s, episode steps: 83, steps per second: 60, episode reward: 83.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.506 [0.000, 1.000], mean observation: -0.022 [-0.908, 0.859], loss: 3.310014, mean_absolute_error: 7.733001, mean_q: 14.397106\n",
      " 3581/5000: episode: 253, duration: 0.279s, episode steps: 17, steps per second: 61, episode reward: 17.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.471 [0.000, 1.000], mean observation: -0.095 [-1.154, 0.774], loss: 3.062480, mean_absolute_error: 7.721941, mean_q: 14.387571\n",
      " 3610/5000: episode: 254, duration: 0.481s, episode steps: 29, steps per second: 60, episode reward: 29.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.483 [0.000, 1.000], mean observation: -0.108 [-1.172, 0.596], loss: 3.203806, mean_absolute_error: 7.713520, mean_q: 14.356073\n",
      " 3641/5000: episode: 255, duration: 0.518s, episode steps: 31, steps per second: 60, episode reward: 31.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.484 [0.000, 1.000], mean observation: -0.124 [-0.703, 0.342], loss: 2.414280, mean_absolute_error: 7.687136, mean_q: 14.426697\n",
      " 3672/5000: episode: 256, duration: 0.516s, episode steps: 31, steps per second: 60, episode reward: 31.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.516 [0.000, 1.000], mean observation: 0.118 [-0.389, 1.036], loss: 3.031850, mean_absolute_error: 7.813119, mean_q: 14.598144\n",
      " 3707/5000: episode: 257, duration: 0.580s, episode steps: 35, steps per second: 60, episode reward: 35.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.514 [0.000, 1.000], mean observation: 0.080 [-0.430, 0.707], loss: 2.732192, mean_absolute_error: 7.733621, mean_q: 14.486419\n",
      " 3741/5000: episode: 258, duration: 0.566s, episode steps: 34, steps per second: 60, episode reward: 34.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.529 [0.000, 1.000], mean observation: 0.080 [-0.378, 0.755], loss: 2.675221, mean_absolute_error: 7.788762, mean_q: 14.649039\n",
      " 3766/5000: episode: 259, duration: 0.417s, episode steps: 25, steps per second: 60, episode reward: 25.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.480 [0.000, 1.000], mean observation: 0.098 [-0.388, 1.117], loss: 3.154772, mean_absolute_error: 7.714794, mean_q: 14.385349\n",
      " 3790/5000: episode: 260, duration: 0.395s, episode steps: 24, steps per second: 61, episode reward: 24.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.116 [-0.538, 0.868], loss: 2.866756, mean_absolute_error: 7.849714, mean_q: 14.646972\n",
      " 3831/5000: episode: 261, duration: 0.681s, episode steps: 41, steps per second: 60, episode reward: 41.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.537 [0.000, 1.000], mean observation: 0.098 [-0.359, 0.774], loss: 2.583478, mean_absolute_error: 7.918097, mean_q: 14.884126\n",
      " 3899/5000: episode: 262, duration: 1.134s, episode steps: 68, steps per second: 60, episode reward: 68.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.515 [0.000, 1.000], mean observation: 0.051 [-0.338, 0.649], loss: 2.517414, mean_absolute_error: 7.935682, mean_q: 14.961006\n",
      " 3943/5000: episode: 263, duration: 0.730s, episode steps: 44, steps per second: 60, episode reward: 44.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.523 [0.000, 1.000], mean observation: 0.108 [-0.262, 0.704], loss: 3.497443, mean_absolute_error: 7.970424, mean_q: 14.848410\n",
      " 3975/5000: episode: 264, duration: 0.530s, episode steps: 32, steps per second: 60, episode reward: 32.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.531 [0.000, 1.000], mean observation: 0.129 [-0.236, 0.699], loss: 2.234648, mean_absolute_error: 8.025439, mean_q: 15.158960\n",
      " 4022/5000: episode: 265, duration: 0.782s, episode steps: 47, steps per second: 60, episode reward: 47.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.489 [0.000, 1.000], mean observation: -0.097 [-1.009, 0.547], loss: 3.111485, mean_absolute_error: 8.084887, mean_q: 15.184868\n",
      " 4074/5000: episode: 266, duration: 0.865s, episode steps: 52, steps per second: 60, episode reward: 52.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.519 [0.000, 1.000], mean observation: 0.073 [-0.345, 0.885], loss: 3.147174, mean_absolute_error: 8.264986, mean_q: 15.578369\n",
      " 4176/5000: episode: 267, duration: 1.695s, episode steps: 102, steps per second: 60, episode reward: 102.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.490 [0.000, 1.000], mean observation: -0.063 [-0.886, 0.591], loss: 3.380391, mean_absolute_error: 8.327658, mean_q: 15.626742\n",
      " 4221/5000: episode: 268, duration: 0.753s, episode steps: 45, steps per second: 60, episode reward: 45.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.467 [0.000, 1.000], mean observation: -0.102 [-0.716, 0.184], loss: 3.042605, mean_absolute_error: 8.278148, mean_q: 15.544637\n",
      " 4279/5000: episode: 269, duration: 0.965s, episode steps: 58, steps per second: 60, episode reward: 58.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.517 [0.000, 1.000], mean observation: 0.075 [-0.404, 0.972], loss: 3.139367, mean_absolute_error: 8.551390, mean_q: 16.125647\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 4317/5000: episode: 270, duration: 0.628s, episode steps: 38, steps per second: 60, episode reward: 38.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.474 [0.000, 1.000], mean observation: -0.101 [-0.926, 0.371], loss: 3.433804, mean_absolute_error: 8.549006, mean_q: 16.095638\n",
      " 4395/5000: episode: 271, duration: 1.298s, episode steps: 78, steps per second: 60, episode reward: 78.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.513 [0.000, 1.000], mean observation: 0.103 [-0.275, 0.768], loss: 2.514771, mean_absolute_error: 8.573539, mean_q: 16.261259\n",
      " 4474/5000: episode: 272, duration: 1.317s, episode steps: 79, steps per second: 60, episode reward: 79.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.481 [0.000, 1.000], mean observation: -0.035 [-0.689, 0.570], loss: 3.321003, mean_absolute_error: 8.714490, mean_q: 16.410271\n",
      " 4534/5000: episode: 273, duration: 0.996s, episode steps: 60, steps per second: 60, episode reward: 60.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.483 [0.000, 1.000], mean observation: -0.079 [-0.790, 0.457], loss: 3.620524, mean_absolute_error: 8.830045, mean_q: 16.552906\n",
      " 4572/5000: episode: 274, duration: 0.633s, episode steps: 38, steps per second: 60, episode reward: 38.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.474 [0.000, 1.000], mean observation: -0.087 [-0.772, 0.222], loss: 4.120203, mean_absolute_error: 8.962638, mean_q: 16.777803\n",
      " 4598/5000: episode: 275, duration: 0.426s, episode steps: 26, steps per second: 61, episode reward: 26.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.096 [-1.098, 0.272], loss: 2.705321, mean_absolute_error: 8.698977, mean_q: 16.424995\n",
      " 4691/5000: episode: 276, duration: 1.553s, episode steps: 93, steps per second: 60, episode reward: 93.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.505 [0.000, 1.000], mean observation: 0.113 [-0.418, 1.171], loss: 2.963197, mean_absolute_error: 8.975530, mean_q: 16.987589\n",
      " 4732/5000: episode: 277, duration: 0.679s, episode steps: 41, steps per second: 60, episode reward: 41.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.488 [0.000, 1.000], mean observation: -0.108 [-0.898, 0.257], loss: 2.958678, mean_absolute_error: 8.950002, mean_q: 16.896894\n",
      " 4774/5000: episode: 278, duration: 0.700s, episode steps: 42, steps per second: 60, episode reward: 42.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.476 [0.000, 1.000], mean observation: -0.099 [-0.709, 0.295], loss: 4.176013, mean_absolute_error: 9.175295, mean_q: 17.231607\n",
      " 4821/5000: episode: 279, duration: 0.780s, episode steps: 47, steps per second: 60, episode reward: 47.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.489 [0.000, 1.000], mean observation: -0.111 [-0.800, 0.232], loss: 2.472282, mean_absolute_error: 9.126007, mean_q: 17.331675\n",
      " 4876/5000: episode: 280, duration: 0.915s, episode steps: 55, steps per second: 60, episode reward: 55.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.527 [0.000, 1.000], mean observation: 0.092 [-0.401, 0.609], loss: 2.832397, mean_absolute_error: 9.209377, mean_q: 17.490622\n",
      " 4993/5000: episode: 281, duration: 1.948s, episode steps: 117, steps per second: 60, episode reward: 117.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.513 [0.000, 1.000], mean observation: 0.113 [-0.352, 0.821], loss: 3.357671, mean_absolute_error: 9.389331, mean_q: 17.769932\n",
      "done, took 83.909 seconds\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x2344c50d518>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Training with verbose and visualization\n",
    "dqn.fit(env, nb_steps=5000, visualize=True, verbose=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing for 20 episodes ...\n",
      "Episode 1: reward: 62.000, steps: 62\n",
      "Episode 2: reward: 64.000, steps: 64\n",
      "Episode 3: reward: 66.000, steps: 66\n",
      "Episode 4: reward: 71.000, steps: 71\n",
      "Episode 5: reward: 50.000, steps: 50\n",
      "Episode 6: reward: 59.000, steps: 59\n",
      "Episode 7: reward: 48.000, steps: 48\n",
      "Episode 8: reward: 36.000, steps: 36\n",
      "Episode 9: reward: 189.000, steps: 189\n",
      "Episode 10: reward: 140.000, steps: 140\n",
      "Episode 11: reward: 72.000, steps: 72\n",
      "Episode 12: reward: 48.000, steps: 48\n",
      "Episode 13: reward: 200.000, steps: 200\n",
      "Episode 14: reward: 75.000, steps: 75\n",
      "Episode 15: reward: 55.000, steps: 55\n",
      "Episode 16: reward: 50.000, steps: 50\n",
      "Episode 17: reward: 73.000, steps: 73\n",
      "Episode 18: reward: 106.000, steps: 106\n",
      "Episode 19: reward: 46.000, steps: 46\n",
      "Episode 20: reward: 79.000, steps: 79\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x23456d4b630>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test the model\n",
    "dqn.test(env, nb_episodes=20, visualize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Train and Test a DQN (100,000 iterations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for 100000 steps ...\n",
      "Interval 1 (0 steps performed)\n",
      "10000/10000 [==============================] - 47s 5ms/step - reward: 1.0000\n",
      "73 episodes - episode_reward: 136.918 [30.000, 200.000] - loss: 5.918 - mean_absolute_error: 15.723 - mean_q: 30.942\n",
      "\n",
      "Interval 2 (10000 steps performed)\n",
      "10000/10000 [==============================] - 46s 5ms/step - reward: 1.0000\n",
      "50 episodes - episode_reward: 197.160 [157.000, 200.000] - loss: 10.537 - mean_absolute_error: 27.565 - mean_q: 55.684\n",
      "\n",
      "Interval 3 (20000 steps performed)\n",
      "10000/10000 [==============================] - 48s 5ms/step - reward: 1.0000\n",
      "53 episodes - episode_reward: 190.226 [130.000, 200.000] - loss: 12.308 - mean_absolute_error: 34.244 - mean_q: 69.635\n",
      "\n",
      "Interval 4 (30000 steps performed)\n",
      "10000/10000 [==============================] - 48s 5ms/step - reward: 1.0000\n",
      "54 episodes - episode_reward: 185.444 [128.000, 200.000] - loss: 12.951 - mean_absolute_error: 37.171 - mean_q: 75.856\n",
      "\n",
      "Interval 5 (40000 steps performed)\n",
      "10000/10000 [==============================] - 48s 5ms/step - reward: 1.0000\n",
      "58 episodes - episode_reward: 172.190 [135.000, 200.000] - loss: 11.832 - mean_absolute_error: 39.074 - mean_q: 80.075\n",
      "\n",
      "Interval 6 (50000 steps performed)\n",
      "10000/10000 [==============================] - 47s 5ms/step - reward: 1.0000\n",
      "53 episodes - episode_reward: 187.226 [11.000, 200.000] - loss: 5.755 - mean_absolute_error: 39.351 - mean_q: 79.483\n",
      "\n",
      "Interval 7 (60000 steps performed)\n",
      "10000/10000 [==============================] - 48s 5ms/step - reward: 1.0000\n",
      "50 episodes - episode_reward: 200.000 [200.000, 200.000] - loss: 3.429 - mean_absolute_error: 36.312 - mean_q: 72.893\n",
      "\n",
      "Interval 8 (70000 steps performed)\n",
      "10000/10000 [==============================] - 47s 5ms/step - reward: 1.0000\n",
      "51 episodes - episode_reward: 195.471 [82.000, 200.000] - loss: 3.814 - mean_absolute_error: 34.300 - mean_q: 68.722\n",
      "\n",
      "Interval 9 (80000 steps performed)\n",
      "10000/10000 [==============================] - 44s 4ms/step - reward: 1.0000\n",
      "52 episodes - episode_reward: 195.000 [94.000, 200.000] - loss: 4.410 - mean_absolute_error: 33.291 - mean_q: 66.578\n",
      "\n",
      "Interval 10 (90000 steps performed)\n",
      "10000/10000 [==============================] - 45s 4ms/step - reward: 1.0000\n",
      "done, took 467.243 seconds\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x23456d665c0>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Training with verbose and visualization\n",
    "dqn.fit(env, nb_steps=100000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing for 20 episodes ...\n",
      "Episode 1: reward: 200.000, steps: 200\n",
      "Episode 2: reward: 200.000, steps: 200\n",
      "Episode 3: reward: 200.000, steps: 200\n",
      "Episode 4: reward: 200.000, steps: 200\n",
      "Episode 5: reward: 200.000, steps: 200\n",
      "Episode 6: reward: 200.000, steps: 200\n",
      "Episode 7: reward: 200.000, steps: 200\n",
      "Episode 8: reward: 200.000, steps: 200\n",
      "Episode 9: reward: 200.000, steps: 200\n",
      "Episode 10: reward: 200.000, steps: 200\n",
      "Episode 11: reward: 200.000, steps: 200\n",
      "Episode 12: reward: 200.000, steps: 200\n",
      "Episode 13: reward: 200.000, steps: 200\n",
      "Episode 14: reward: 200.000, steps: 200\n",
      "Episode 15: reward: 200.000, steps: 200\n",
      "Episode 16: reward: 200.000, steps: 200\n",
      "Episode 17: reward: 200.000, steps: 200\n",
      "Episode 18: reward: 200.000, steps: 200\n",
      "Episode 19: reward: 200.000, steps: 200\n",
      "Episode 20: reward: 200.000, steps: 200\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x23456d66828>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test the model\n",
    "dqn.test(env, nb_episodes=20, visualize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion: \n",
    "\n",
    "* A DQN trained only 5000 times performs fairly well, managing to earn a max reward of 200- a perfect score- on a single iteration. However, the agent only managed to exceed 100 time steps 3 of 20 times.\n",
    "* A DQN trained 100,000 times, on the other hand, performed exceedingly well. In this scenario, the agent managed to reach the maximum number of steps per episode, 200, each episode."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
